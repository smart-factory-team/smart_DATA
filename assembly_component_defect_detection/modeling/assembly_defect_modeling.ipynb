{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 드라이브 마운트"
      ],
      "metadata": {
        "id": "mHWLD4Fr1MBS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "c5cOJ-3Si0yk",
        "outputId": "e1ae8a42-b6a1-4a18-f3b7-81bc4981561c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라이브러리 임포트"
      ],
      "metadata": {
        "id": "dFpmhBi_1N7D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z--c93OvcWVG"
      },
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 import\n",
        "import os\n",
        "import json\n",
        "import datetime\n",
        "import shutil\n",
        "import time\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAgE9YV5demZ"
      },
      "source": [
        "## 중간 모델 저장 장치 반영된 모델링 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzD7e7kebUlv"
      },
      "outputs": [],
      "source": [
        "# 매핑 파일을 사용하는 수정된 ImageDataset 클래스\n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class ImageDatasetWithMapping(Dataset):\n",
        "    def __init__(self, data_dir, mapping_file_path, class_names, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.class_names = class_names\n",
        "        self.transform = transform\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_names)}\n",
        "\n",
        "        # 매핑 파일 로드\n",
        "        with open(mapping_file_path, 'r', encoding='utf-8') as f:\n",
        "            self.filename_to_label = json.load(f)\n",
        "\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        self._load_from_mapping()\n",
        "\n",
        "    def _load_from_mapping(self):\n",
        "        \"\"\"매핑 파일을 사용해서 이미지와 라벨 로드\"\"\"\n",
        "        valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
        "\n",
        "        for file_name in os.listdir(self.data_dir):\n",
        "            if file_name.lower().endswith(valid_extensions):\n",
        "                # 매핑 파일에서 라벨 찾기\n",
        "                if file_name in self.filename_to_label:\n",
        "                    label = self.filename_to_label[file_name]\n",
        "\n",
        "                    # 클래스 인덱스 확인\n",
        "                    if label in self.class_to_idx:\n",
        "                        image_path = os.path.join(self.data_dir, file_name)\n",
        "                        self.image_paths.append(image_path)\n",
        "                        self.labels.append(self.class_to_idx[label])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "            # 에러 발생시 기본 이미지 생성\n",
        "            image = Image.new('RGB', (224, 224), color='white')\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# 수정된 create_data_loaders 함수\n",
        "def create_data_loaders_with_mapping(class_names):\n",
        "    \"\"\"매핑 파일을 사용하는 데이터 로더 생성 함수\"\"\"\n",
        "    from torchvision import transforms\n",
        "    from torch.utils.data import DataLoader\n",
        "    import torch\n",
        "\n",
        "    # Config 클래스 (기존과 동일)\n",
        "    dataset_dir = '/content/drive/MyDrive/의장공정/데이터/image_classification/dataset'\n",
        "\n",
        "    # 데이터 변환 정의\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomCrop((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # 데이터셋 생성\n",
        "    loaders = {}\n",
        "\n",
        "    for split in ['train', 'valid', 'test']:\n",
        "        split_dir = os.path.join(dataset_dir, split)\n",
        "        mapping_file_path = f'/content/{split}_mapping.json'\n",
        "\n",
        "        if os.path.exists(split_dir) and os.path.exists(mapping_file_path):\n",
        "            dataset = ImageDatasetWithMapping(\n",
        "                data_dir=split_dir,\n",
        "                mapping_file_path=mapping_file_path,\n",
        "                class_names=class_names,\n",
        "                transform=train_transform if split == 'train' else test_transform\n",
        "            )\n",
        "\n",
        "            if len(dataset) > 0:\n",
        "                loaders[split] = DataLoader(\n",
        "                    dataset,\n",
        "                    batch_size=32,\n",
        "                    shuffle=(split == 'train'),\n",
        "                    num_workers=2,\n",
        "                    pin_memory=True if torch.cuda.is_available() else False\n",
        "                )\n",
        "\n",
        "                print(f\"{split.capitalize()} dataset: {len(dataset)} samples\")\n",
        "\n",
        "    return loaders.get('train'), loaders.get('valid'), loaders.get('test')\n",
        "\n",
        "\n",
        "\n",
        "def create_model(num_classes):\n",
        "    \"\"\"ResNet50 모델을 생성합니다.\"\"\"\n",
        "    print(f\"모델 생성 중... (클래스 수: {num_classes})\")\n",
        "\n",
        "    # pretrained=True로 사전 훈련된 가중치 사용\n",
        "    model = models.resnet50(pretrained=True)\n",
        "\n",
        "    # 마지막 fully connected layer를 클래스 수에 맞게 교체\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_model(model, train_loader, valid_loader, num_classes):\n",
        "    \"\"\"모델을 학습합니다.\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=Config.lr)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
        "\n",
        "    # 학습 기록\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "\n",
        "    best_valid_acc = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print(\"학습 시작...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(Config.num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{Config.num_epochs}')\n",
        "        print('-' * 50)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_train_acc = running_corrects.double() / len(train_loader.dataset)\n",
        "\n",
        "        # Validation phase\n",
        "        if valid_loader is not None:\n",
        "            model.eval()\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in valid_loader:\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_valid_loss = running_loss / len(valid_loader.dataset)\n",
        "            epoch_valid_acc = running_corrects.double() / len(valid_loader.dataset)\n",
        "\n",
        "            print(f'Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f}')\n",
        "            print(f'Valid Loss: {epoch_valid_loss:.4f} Acc: {epoch_valid_acc:.4f}')\n",
        "\n",
        "            # 기록 저장\n",
        "            valid_losses.append(epoch_valid_loss)\n",
        "            valid_accs.append(epoch_valid_acc.cpu().numpy())\n",
        "\n",
        "            # Best model 저장\n",
        "            if epoch_valid_acc > best_valid_acc:\n",
        "                best_valid_acc = epoch_valid_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        else:\n",
        "            print(f'Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f}')\n",
        "\n",
        "        # 기록 저장\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accs.append(epoch_train_acc.cpu().numpy())\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # 최적 모델 로드\n",
        "    if valid_loader is not None:\n",
        "        model.load_state_dict(best_model_wts)\n",
        "\n",
        "    # 모델 저장\n",
        "    model_path = os.path.join(Config.model_dir, f'best_model_{Config.timestamp}.pth')\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f'\\n최적 모델 저장: {model_path}')\n",
        "    if valid_loader is not None:\n",
        "        print(f'최고 검증 정확도: {best_valid_acc:.4f}')\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f'학습 완료 시간: {training_time/60:.2f}분')\n",
        "\n",
        "    return model, train_losses, train_accs, valid_losses, valid_accs\n",
        "\n",
        "def evaluate_model(model, test_loader, class_names):\n",
        "    \"\"\"모델을 평가합니다.\"\"\"\n",
        "    if test_loader is None:\n",
        "        print(\"테스트 데이터가 없어 평가를 건너뜁니다.\")\n",
        "        return None\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # 평가 지표 계산\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
        "    f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Classification Report\n",
        "    report = classification_report(\n",
        "        all_labels,\n",
        "        all_preds,\n",
        "        target_names=class_names,\n",
        "        output_dict=True\n",
        "    )\n",
        "\n",
        "    print(f'\\n=== 테스트 결과 ===')\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "    print(f'F1-Score (Macro): {f1_macro:.4f}')\n",
        "    print(f'F1-Score (Micro): {f1_micro:.4f}')\n",
        "\n",
        "    # 결과 저장\n",
        "    results = {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_macro': f1_macro,\n",
        "        'f1_micro': f1_micro,\n",
        "        'confusion_matrix': cm.tolist(),\n",
        "        'classification_report': report,\n",
        "        'timestamp': Config.timestamp\n",
        "    }\n",
        "\n",
        "    result_path = os.path.join(Config.result_dir, f'test_results_{Config.timestamp}.json')\n",
        "    with open(result_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f'결과 저장: {result_path}')\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UVq0_mdjdvB1",
        "outputId": "e748c66a-60d5-4029-b918-4c40dcd04427"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "체크포인트 저장 기능이 추가된 학습 시스템이 준비되었습니다!\n",
            "매 5 에포크마다 자동으로 저장됩니다.\n",
            "런타임이 끊어져도 마지막 체크포인트에서 재개할 수 있습니다!\n",
            "Train dataset: 15076 samples\n",
            "Valid dataset: 3232 samples\n",
            "Test dataset: 3245 samples\n",
            "체크포인트 발견: /content/drive/MyDrive/의장공정/데이터/image_classification/checkpoints/latest_checkpoint_250802_1554.pth\n",
            "체크포인트에서 재개하시겠습니까? (y/n): y\n",
            "모델 생성 중... (클래스 수: 24)\n",
            "Using device: cuda\n",
            "체크포인트 로드: /content/drive/MyDrive/의장공정/데이터/image_classification/checkpoints/latest_checkpoint_250802_1554.pth\n",
            "에포크 14부터 학습 재개\n",
            "학습 재개... (에포크 15부터)\n",
            "\n",
            "Epoch 16/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.1749 Acc: 0.9388\n",
            "Valid Loss: 0.8084 Acc: 0.7509\n",
            "새로운 최고 성능! Valid Acc: 0.7509\n",
            "\n",
            "Epoch 17/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.1286 Acc: 0.9583\n",
            "Valid Loss: 0.8225 Acc: 0.7559\n",
            "새로운 최고 성능! Valid Acc: 0.7559\n",
            "\n",
            "Epoch 18/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.1059 Acc: 0.9645\n",
            "Valid Loss: 0.8324 Acc: 0.7571\n",
            "새로운 최고 성능! Valid Acc: 0.7571\n",
            "\n",
            "Epoch 19/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0931 Acc: 0.9693\n",
            "Valid Loss: 0.8670 Acc: 0.7624\n",
            "새로운 최고 성능! Valid Acc: 0.7624\n",
            "\n",
            "Epoch 20/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0838 Acc: 0.9724\n",
            "Valid Loss: 0.9215 Acc: 0.7562\n",
            "체크포인트 저장: /content/drive/MyDrive/의장공정/데이터/image_classification/checkpoints/checkpoint_epoch_19_250802_1554.pth\n",
            "에포크 20 체크포인트 저장 완료\n",
            "\n",
            "Epoch 21/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0810 Acc: 0.9730\n",
            "Valid Loss: 0.9260 Acc: 0.7630\n",
            "새로운 최고 성능! Valid Acc: 0.7630\n",
            "\n",
            "Epoch 22/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0712 Acc: 0.9767\n",
            "Valid Loss: 0.9237 Acc: 0.7633\n",
            "새로운 최고 성능! Valid Acc: 0.7633\n",
            "\n",
            "Epoch 23/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0653 Acc: 0.9797\n",
            "Valid Loss: 0.9359 Acc: 0.7587\n",
            "\n",
            "Epoch 24/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0626 Acc: 0.9806\n",
            "Valid Loss: 0.9779 Acc: 0.7605\n",
            "\n",
            "Epoch 25/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.0558 Acc: 0.9828\n",
            "Valid Loss: 0.9865 Acc: 0.7574\n",
            "체크포인트 저장: /content/drive/MyDrive/의장공정/데이터/image_classification/checkpoints/checkpoint_epoch_24_250802_1554.pth\n",
            "에포크 25 체크포인트 저장 완료\n",
            "\n",
            "Epoch 26/50\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x78ac33e3f9c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x78ac33e3f9c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3419846280.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;31m# 7. 모델 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training_with_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3419846280.py\u001b[0m in \u001b[0;36mrun_training_with_checkpoints\u001b[0;34m()\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;31m# 학습 시작\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m     trained_model, train_losses, train_accs, valid_losses, valid_accs = train_model_with_checkpoints(\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     )\n",
            "\u001b[0;32m/tmp/ipython-input-3419846280.py\u001b[0m in \u001b[0;36mtrain_model_with_checkpoints\u001b[0;34m(model, train_loader, valid_loader, num_classes, resume_from_checkpoint)\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 매핑 파일을 사용하는 수정된 ImageDataset 클래스\n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class ImageDatasetWithMapping(Dataset):\n",
        "    def __init__(self, data_dir, mapping_file_path, class_names, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.class_names = class_names\n",
        "        self.transform = transform\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_names)}\n",
        "\n",
        "        # 매핑 파일 로드\n",
        "        with open(mapping_file_path, 'r', encoding='utf-8') as f:\n",
        "            self.filename_to_label = json.load(f)\n",
        "\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        self._load_from_mapping()\n",
        "\n",
        "    def _load_from_mapping(self):\n",
        "        \"\"\"매핑 파일을 사용해서 이미지와 라벨 로드\"\"\"\n",
        "        valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
        "\n",
        "        for file_name in os.listdir(self.data_dir):\n",
        "            if file_name.lower().endswith(valid_extensions):\n",
        "                # 매핑 파일에서 라벨 찾기\n",
        "                if file_name in self.filename_to_label:\n",
        "                    label = self.filename_to_label[file_name]\n",
        "\n",
        "                    # 클래스 인덱스 확인\n",
        "                    if label in self.class_to_idx:\n",
        "                        image_path = os.path.join(self.data_dir, file_name)\n",
        "                        self.image_paths.append(image_path)\n",
        "                        self.labels.append(self.class_to_idx[label])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "            # 에러 발생시 기본 이미지 생성\n",
        "            image = Image.new('RGB', (224, 224), color='white')\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# 수정된 create_data_loaders 함수\n",
        "def create_data_loaders_with_mapping(class_names):\n",
        "    \"\"\"매핑 파일을 사용하는 데이터 로더 생성 함수\"\"\"\n",
        "    from torchvision import transforms\n",
        "    from torch.utils.data import DataLoader\n",
        "    import torch\n",
        "\n",
        "    # Config 클래스 (기존과 동일)\n",
        "    dataset_dir = '/content/drive/MyDrive/의장공정/데이터/image_classification/dataset'\n",
        "\n",
        "    # 데이터 변환 정의\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomCrop((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # 데이터셋 생성\n",
        "    loaders = {}\n",
        "\n",
        "    for split in ['train', 'valid', 'test']:\n",
        "        split_dir = os.path.join(dataset_dir, split)\n",
        "        mapping_file_path = f'/content/{split}_mapping.json'\n",
        "\n",
        "        if os.path.exists(split_dir) and os.path.exists(mapping_file_path):\n",
        "            dataset = ImageDatasetWithMapping(\n",
        "                data_dir=split_dir,\n",
        "                mapping_file_path=mapping_file_path,\n",
        "                class_names=class_names,\n",
        "                transform=train_transform if split == 'train' else test_transform\n",
        "            )\n",
        "\n",
        "            if len(dataset) > 0:\n",
        "                loaders[split] = DataLoader(\n",
        "                    dataset,\n",
        "                    batch_size=32,\n",
        "                    shuffle=(split == 'train'),\n",
        "                    num_workers=2,\n",
        "                    pin_memory=True if torch.cuda.is_available() else False\n",
        "                )\n",
        "\n",
        "                print(f\"{split.capitalize()} dataset: {len(dataset)} samples\")\n",
        "\n",
        "    return loaders.get('train'), loaders.get('valid'), loaders.get('test')\n",
        "\n",
        "def create_model(num_classes):\n",
        "    \"\"\"ResNet50 모델을 생성합니다.\"\"\"\n",
        "    print(f\"모델 생성 중... (클래스 수: {num_classes})\")\n",
        "\n",
        "    # pretrained=True로 사전 훈련된 가중치 사용\n",
        "    model = models.resnet50(pretrained=True)\n",
        "\n",
        "    # 마지막 fully connected layer를 클래스 수에 맞게 교체\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_model(model, train_loader, valid_loader, num_classes):\n",
        "    \"\"\"모델을 학습합니다.\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=Config.lr)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
        "\n",
        "    # 학습 기록\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "\n",
        "    best_valid_acc = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print(\"학습 시작...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(Config.num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{Config.num_epochs}')\n",
        "        print('-' * 50)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_train_acc = running_corrects.double() / len(train_loader.dataset)\n",
        "\n",
        "        # Validation phase\n",
        "        if valid_loader is not None:\n",
        "            model.eval()\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in valid_loader:\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_valid_loss = running_loss / len(valid_loader.dataset)\n",
        "            epoch_valid_acc = running_corrects.double() / len(valid_loader.dataset)\n",
        "\n",
        "            print(f'Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f}')\n",
        "            print(f'Valid Loss: {epoch_valid_loss:.4f} Acc: {epoch_valid_acc:.4f}')\n",
        "\n",
        "            # 기록 저장\n",
        "            valid_losses.append(epoch_valid_loss)\n",
        "            valid_accs.append(epoch_valid_acc.cpu().numpy())\n",
        "\n",
        "            # Best model 저장\n",
        "            if epoch_valid_acc > best_valid_acc:\n",
        "                best_valid_acc = epoch_valid_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        else:\n",
        "            print(f'Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f}')\n",
        "\n",
        "        # 기록 저장\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accs.append(epoch_train_acc.cpu().numpy())\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # 최적 모델 로드\n",
        "    if valid_loader is not None:\n",
        "        model.load_state_dict(best_model_wts)\n",
        "\n",
        "    # 모델 저장\n",
        "    model_path = os.path.join(Config.model_dir, f'best_model_{Config.timestamp}.pth')\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f'\\n최적 모델 저장: {model_path}')\n",
        "    if valid_loader is not None:\n",
        "        print(f'최고 검증 정확도: {best_valid_acc:.4f}')\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f'학습 완료 시간: {training_time/60:.2f}분')\n",
        "\n",
        "    return model, train_losses, train_accs, valid_losses, valid_accs\n",
        "\n",
        "def evaluate_model(model, test_loader, class_names):\n",
        "    \"\"\"모델을 평가합니다.\"\"\"\n",
        "    if test_loader is None:\n",
        "        print(\"테스트 데이터가 없어 평가를 건너뜁니다.\")\n",
        "        return None\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # 평가 지표 계산\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
        "    f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Classification Report\n",
        "    report = classification_report(\n",
        "        all_labels,\n",
        "        all_preds,\n",
        "        target_names=class_names,\n",
        "        output_dict=True\n",
        "    )\n",
        "\n",
        "    print(f'\\n=== 테스트 결과 ===')\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "    print(f'F1-Score (Macro): {f1_macro:.4f}')\n",
        "    print(f'F1-Score (Micro): {f1_micro:.4f}')\n",
        "\n",
        "    # 결과 저장\n",
        "    results = {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_macro': f1_macro,\n",
        "        'f1_micro': f1_micro,\n",
        "        'confusion_matrix': cm.tolist(),\n",
        "        'classification_report': report,\n",
        "        'timestamp': Config.timestamp\n",
        "    }\n",
        "\n",
        "    result_path = os.path.join(Config.result_dir, f'test_results_{Config.timestamp}.json')\n",
        "    with open(result_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f'결과 저장: {result_path}')\n",
        "\n",
        "    return results\n",
        "\n",
        "# 체크포인트 저장 기능이 추가된 개선된 Config 클래스\n",
        "class Config:\n",
        "    # 경로 설정\n",
        "    work_dir = '/content/drive/MyDrive/의장공정/데이터/image_classification'\n",
        "    dataset_dir = os.path.join(work_dir, 'dataset')\n",
        "    model_dir = os.path.join(work_dir, 'models')\n",
        "    result_dir = os.path.join(work_dir, 'results')\n",
        "    checkpoint_dir = os.path.join(work_dir, 'checkpoints')  # 체크포인트 저장 경로\n",
        "\n",
        "    # 학습 설정\n",
        "    IMG_SIZE = (224, 224)\n",
        "    batch_size = 32\n",
        "    num_workers = 2\n",
        "    lr = 1e-4\n",
        "    num_epochs = 50\n",
        "\n",
        "    # 체크포인트 설정\n",
        "    save_checkpoint_every = 5  # 5 에포크마다 저장\n",
        "\n",
        "    # 현재 시간\n",
        "    now = datetime.datetime.now()\n",
        "    timestamp = '250802_1554'\n",
        "\n",
        "# 체크포인트 저장/로드 함수들\n",
        "def save_checkpoint(model, optimizer, scheduler, epoch, train_losses, train_accs,\n",
        "                   valid_losses, valid_accs, best_valid_acc, checkpoint_dir, timestamp):\n",
        "    \"\"\"체크포인트 저장\"\"\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'train_losses': train_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'valid_losses': valid_losses,\n",
        "        'valid_accs': valid_accs,\n",
        "        'best_valid_acc': best_valid_acc,\n",
        "        'timestamp': timestamp\n",
        "    }\n",
        "\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}_{timestamp}.pth')\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f'체크포인트 저장: {checkpoint_path}')\n",
        "\n",
        "    # 최신 체크포인트 경로도 별도 저장 (복구 시 쉽게 찾기 위해)\n",
        "    latest_path = os.path.join(checkpoint_dir, f'latest_checkpoint_{timestamp}.pth')\n",
        "    torch.save(checkpoint, latest_path)\n",
        "\n",
        "    return checkpoint_path\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer, scheduler):\n",
        "    \"\"\"체크포인트 로드\"\"\"\n",
        "    print(f\"체크포인트 로드: {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    return (checkpoint['epoch'], checkpoint['train_losses'], checkpoint['train_accs'],\n",
        "            checkpoint['valid_losses'], checkpoint['valid_accs'], checkpoint['best_valid_acc'])\n",
        "\n",
        "def find_latest_checkpoint(checkpoint_dir, timestamp):\n",
        "    \"\"\"가장 최근 체크포인트 찾기\"\"\"\n",
        "    latest_path = os.path.join(checkpoint_dir, f'latest_checkpoint_{timestamp}.pth')\n",
        "    if os.path.exists(latest_path):\n",
        "        return latest_path\n",
        "\n",
        "    # 해당 타임스탬프의 체크포인트들 찾기\n",
        "    checkpoints = []\n",
        "    if os.path.exists(checkpoint_dir):\n",
        "        for file in os.listdir(checkpoint_dir):\n",
        "            if file.startswith('checkpoint_epoch_') and timestamp in file:\n",
        "                epoch_num = int(file.split('_')[2])\n",
        "                checkpoints.append((epoch_num, os.path.join(checkpoint_dir, file)))\n",
        "\n",
        "    if checkpoints:\n",
        "        checkpoints.sort(reverse=True)  # 에포크 번호 내림차순 정렬\n",
        "        return checkpoints[0][1]  # 가장 최근 체크포인트 반환\n",
        "\n",
        "    return None\n",
        "\n",
        "# 개선된 학습 함수 (체크포인트 기능 포함)\n",
        "def train_model_with_checkpoints(model, train_loader, valid_loader, num_classes,\n",
        "                                resume_from_checkpoint=None):\n",
        "    \"\"\"체크포인트 저장 기능이 있는 모델 학습\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=Config.lr)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
        "\n",
        "    # 학습 기록 초기화\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "    best_valid_acc = 0.0\n",
        "    start_epoch = 0\n",
        "\n",
        "    # 체크포인트에서 복구\n",
        "    if resume_from_checkpoint:\n",
        "        try:\n",
        "            (start_epoch, train_losses, train_accs, valid_losses,\n",
        "             valid_accs, best_valid_acc) = load_checkpoint(resume_from_checkpoint, model, optimizer, scheduler)\n",
        "            print(f\"에포크 {start_epoch}부터 학습 재개\")\n",
        "            start_epoch += 1  # 다음 에포크부터 시작\n",
        "        except Exception as e:\n",
        "            print(f\"체크포인트 로드 실패: {e}\")\n",
        "            print(\"처음부터 학습을 시작합니다.\")\n",
        "            start_epoch = 0\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print(\"학습 시작...\" if start_epoch == 0 else f\"학습 재개... (에포크 {start_epoch}부터)\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(start_epoch, Config.num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{Config.num_epochs}')\n",
        "        print('-' * 50)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_train_acc = running_corrects.double() / len(train_loader.dataset)\n",
        "\n",
        "        # Validation phase\n",
        "        if valid_loader is not None:\n",
        "            model.eval()\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in valid_loader:\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_valid_loss = running_loss / len(valid_loader.dataset)\n",
        "            epoch_valid_acc = running_corrects.double() / len(valid_loader.dataset)\n",
        "\n",
        "            print(f'Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f}')\n",
        "            print(f'Valid Loss: {epoch_valid_loss:.4f} Acc: {epoch_valid_acc:.4f}')\n",
        "\n",
        "            # 기록 저장\n",
        "            valid_losses.append(epoch_valid_loss)\n",
        "            valid_accs.append(epoch_valid_acc.cpu().numpy())\n",
        "\n",
        "            # Best model 저장\n",
        "            if epoch_valid_acc > best_valid_acc:\n",
        "                best_valid_acc = epoch_valid_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                print(f\"새로운 최고 성능! Valid Acc: {best_valid_acc:.4f}\")\n",
        "        else:\n",
        "            print(f'Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f}')\n",
        "\n",
        "        # 기록 저장\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accs.append(epoch_train_acc.cpu().numpy())\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # 체크포인트 저장 (매 N 에포크마다)\n",
        "        if (epoch + 1) % Config.save_checkpoint_every == 0:\n",
        "            save_checkpoint(model, optimizer, scheduler, epoch, train_losses, train_accs,\n",
        "                          valid_losses, valid_accs, best_valid_acc, Config.checkpoint_dir, Config.timestamp)\n",
        "            print(f\"에포크 {epoch+1} 체크포인트 저장 완료\")\n",
        "\n",
        "    # 최적 모델 로드\n",
        "    if valid_loader is not None:\n",
        "        model.load_state_dict(best_model_wts)\n",
        "\n",
        "    # 최종 모델 저장\n",
        "    os.makedirs(Config.model_dir, exist_ok=True)\n",
        "    model_path = os.path.join(Config.model_dir, f'best_model_{Config.timestamp}.pth')\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f'\\n최적 모델 저장: {model_path}')\n",
        "    if valid_loader is not None:\n",
        "        print(f'최고 검증 정확도: {best_valid_acc:.4f}')\n",
        "\n",
        "    # 최종 체크포인트도 저장\n",
        "    save_checkpoint(model, optimizer, scheduler, Config.num_epochs-1, train_losses, train_accs,\n",
        "                   valid_losses, valid_accs, best_valid_acc, Config.checkpoint_dir, Config.timestamp)\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f'학습 완료 시간: {training_time/60:.2f}분')\n",
        "\n",
        "    return model, train_losses, train_accs, valid_losses, valid_accs\n",
        "\n",
        "# 체크포인트 복구를 위한 헬퍼 함수\n",
        "def resume_training_from_latest():\n",
        "    \"\"\"가장 최근 체크포인트에서 학습 재개\"\"\"\n",
        "    latest_checkpoint = find_latest_checkpoint(Config.checkpoint_dir, Config.timestamp)\n",
        "\n",
        "    if latest_checkpoint:\n",
        "        print(f\"체크포인트 발견: {latest_checkpoint}\")\n",
        "        return latest_checkpoint\n",
        "    else:\n",
        "        print(\"체크포인트를 찾을 수 없습니다. 처음부터 시작합니다.\")\n",
        "        return None\n",
        "\n",
        "# 수정된 실행 함수\n",
        "def run_training_with_checkpoints():\n",
        "    \"\"\"체크포인트 기능을 포함한 학습 실행\"\"\"\n",
        "\n",
        "    # 체크포인트 디렉토리 생성\n",
        "    os.makedirs(Config.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # 최근 체크포인트 확인\n",
        "    resume_checkpoint = resume_training_from_latest()\n",
        "\n",
        "    if resume_checkpoint:\n",
        "        response = input(\"체크포인트에서 재개하시겠습니까? (y/n): \")\n",
        "        if response.lower() != 'y':\n",
        "            resume_checkpoint = None\n",
        "\n",
        "    # 모델 생성\n",
        "    model = create_model(len(class_names))\n",
        "\n",
        "    # 학습 시작\n",
        "    trained_model, train_losses, train_accs, valid_losses, valid_accs = train_model_with_checkpoints(\n",
        "        model, train_loader, valid_loader, len(class_names), resume_checkpoint\n",
        "    )\n",
        "\n",
        "    # 평가\n",
        "    results = evaluate_model(trained_model, test_loader, class_names)\n",
        "\n",
        "    return trained_model, results\n",
        "\n",
        "print(\"체크포인트 저장 기능이 추가된 학습 시스템이 준비되었습니다!\")\n",
        "print(f\"매 {Config.save_checkpoint_every} 에포크마다 자동으로 저장됩니다.\")\n",
        "print(\"런타임이 끊어져도 마지막 체크포인트에서 재개할 수 있습니다!\")\n",
        "\n",
        "class_names = [\n",
        "    '고정 불량_불량품', '고정 불량_양품', '고정핀 불량_불량품', '고정핀 불량_양품',\n",
        "    '단차_불량품', '단차_양품', '스크래치_불량품', '스크래치_양품',\n",
        "    '실링 불량_불량품', '실링 불량_양품', '연계 불량_불량품', '연계 불량_양품',\n",
        "    '외관 손상_불량품', '외관 손상_양품', '유격 불량_불량품', '유격 불량_양품',\n",
        "    '장착 불량_불량품', '장착 불량_양품', '체결 불량_불량품', '체결 불량_양품',\n",
        "    '헤밍 불량_불량품', '헤밍 불량_양품', '홀 변형_불량품', '홀 변형_양품'\n",
        "]\n",
        "\n",
        "train_loader, valid_loader, test_loader = create_data_loaders_with_mapping(class_names)\n",
        "\n",
        "# 7. 모델 학습\n",
        "trained_model, results = run_training_with_checkpoints()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tdBIK_bfAgo",
        "outputId": "e4a793d5-e0c1-4aec-b05e-34f8f73a0546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 1106029\n",
            "-rw------- 1 root root 283144042 Aug  3 09:50 checkpoint_epoch_14_250802_1554.pth\n",
            "-rw------- 1 root root 283141930 Aug  2 22:00 checkpoint_epoch_4_250802_1554.pth\n",
            "-rw------- 1 root root 283143018 Aug  3 03:50 checkpoint_epoch_9_250802_1554.pth\n",
            "-rw------- 1 root root 283144042 Aug  3 09:50 latest_checkpoint_250802_1554.pth\n"
          ]
        }
      ],
      "source": [
        "!ls -la /content/drive/MyDrive/의장공정/데이터/image_classification/checkpoints/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaerFGii3TfR",
        "outputId": "7e20d675-a761-47e7-c3a7-00166cbe995e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "저장된 에포크: 14\n",
            "최고 검증 정확도: 0.7301980198019802\n",
            "학습 기록 길이: 15\n"
          ]
        }
      ],
      "source": [
        "# weights_only=False 옵션 추가\n",
        "checkpoint = torch.load('/content/drive/MyDrive/의장공정/데이터/image_classification/checkpoints/latest_checkpoint_250802_1554.pth', weights_only=False)\n",
        "print(f\"저장된 에포크: {checkpoint['epoch']}\")\n",
        "print(f\"최고 검증 정확도: {checkpoint['best_valid_acc']}\")\n",
        "print(f\"학습 기록 길이: {len(checkpoint['train_losses'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv0JEHDoeL8r"
      },
      "source": [
        "## 고도화 1:  과적합 방지 장치 추가\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTkr_ThW5ek3",
        "outputId": "d74c4784-e751-4f1c-fe70-f86802659252"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU: Tesla T4\n",
            "새 실험 시작 - 타임스탬프: 250804_2305\n",
            "저장 폴더: models_v2, results_v2, checkpoints_v2\n",
            "=== 새 실험 준비 완료 ===\n",
            "다음 단계:\n",
            "1. train_loader, valid_loader, test_loader = create_robust_loaders(class_names)\n",
            "2. model = create_anti_overfit_model(len(class_names))\n",
            "3. trained_model, losses, accs, val_losses, val_accs = train_anti_overfit_model(model, train_loader, valid_loader, len(class_names))\n",
            "\n",
            "주요 개선사항:\n",
            "- 새 폴더로 완전히 새 시작\n",
            "- 매우 강한 정규화 (Dropout 0.7, Weight Decay, Label Smoothing)\n",
            "- 강화된 데이터 증강\n",
            "- Early Stopping\n",
            "- 대부분 레이어 고정 (layer4만 학습)\n",
            "- 작은 배치 크기 (12)\n",
            "- 낮은 학습률 (3e-5)\n",
            "로드된 이미지: 15076개\n",
            "train: 15076 samples\n",
            "로드된 이미지: 3232개\n",
            "valid: 3232 samples\n",
            "로드된 이미지: 3245개\n",
            "test: 3245 samples\n",
            "과적합 방지 모델 생성 (클래스: 24)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 과적합 방지 학습 시작 ===\n",
            "학습률: 3e-05, 배치크기: 12\n",
            "Weight Decay: 0.0001, Dropout: 0.7\n",
            "\n",
            "Epoch 1/40\n",
            "------------------------------------------------------------\n",
            "Train Loss: 2.9595 Acc: 0.1482\n",
            "Valid Loss: 2.3569 Acc: 0.3159\n",
            "과적합 정도: -0.1677 | 시간: 16878.1초\n",
            "현재 학습률: 3.00e-05\n",
            "★ 새로운 최고 성능! Valid Acc: 0.3159\n",
            "\n",
            "Epoch 2/40\n",
            "------------------------------------------------------------\n",
            "Train Loss: 2.4502 Acc: 0.2885\n",
            "Valid Loss: 1.9853 Acc: 0.3957\n",
            "과적합 정도: -0.1073 | 시간: 10862.7초\n",
            "현재 학습률: 3.00e-05\n",
            "★ 새로운 최고 성능! Valid Acc: 0.3957\n",
            "\n",
            "Epoch 3/40\n",
            "------------------------------------------------------------\n",
            "이미지 오류: /content/drive/MyDrive/의장공정/데이터/image_classification/dataset/train/204_101_10_4a61c180-d8ef-4311-923e-0ebea02a1368.jpg\n",
            "Train Loss: 2.2265 Acc: 0.3408\n",
            "Valid Loss: 1.8157 Acc: 0.4542\n",
            "과적합 정도: -0.1134 | 시간: 11747.6초\n",
            "현재 학습률: 3.00e-05\n",
            "★ 새로운 최고 성능! Valid Acc: 0.4542\n",
            "✓ 체크포인트 저장: epoch 3\n",
            "\n",
            "Epoch 4/40\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# 새 폴더로 완전히 처음부터 시작하는 코드\n",
        "\n",
        "import warnings\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import datetime\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# PIL 설정\n",
        "warnings.filterwarnings(\"ignore\", category=Image.DecompressionBombWarning)\n",
        "Image.MAX_IMAGE_PIXELS = None\n",
        "\n",
        "# GPU 설정\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    print(\"CPU 모드\")\n",
        "\n",
        "# 새로운 Config - 과적합 방지 강화\n",
        "class ConfigV2:\n",
        "    # 새로운 경로 설정\n",
        "    work_dir = '/content/drive/MyDrive/의장공정/데이터/image_classification'\n",
        "    dataset_dir = os.path.join(work_dir, 'dataset')\n",
        "\n",
        "    # 새 폴더들 - v2로 구분\n",
        "    model_dir = os.path.join(work_dir, 'models_v2')\n",
        "    result_dir = os.path.join(work_dir, 'results_v2')\n",
        "    checkpoint_dir = os.path.join(work_dir, 'checkpoints_v2')\n",
        "\n",
        "    # 과적합 방지를 위한 학습 설정\n",
        "    IMG_SIZE = (224, 224)\n",
        "    batch_size = 12  # 더 작게\n",
        "    num_workers = 0  # 멀티프로세싱 비활성화\n",
        "    lr = 3e-5  # 더 낮은 학습률\n",
        "    num_epochs = 40  # 더 적은 에포크\n",
        "\n",
        "    # 정규화 설정\n",
        "    weight_decay = 1e-4  # L2 정규화\n",
        "    dropout_rate = 0.7\n",
        "\n",
        "    # 체크포인트 설정\n",
        "    save_checkpoint_every = 3\n",
        "\n",
        "    # 새로운 타임스탬프\n",
        "    now = datetime.datetime.now()\n",
        "    timestamp = now.strftime('%y%m%d_%H%M')  # 현재 시간으로\n",
        "\n",
        "    # Early stopping 설정\n",
        "    patience = 8  # 8 에포크 동안 개선 없으면 중단\n",
        "    min_delta = 0.001  # 최소 개선 임계값\n",
        "\n",
        "print(f\"새 실험 시작 - 타임스탬프: {ConfigV2.timestamp}\")\n",
        "print(f\"저장 폴더: models_v2, results_v2, checkpoints_v2\")\n",
        "\n",
        "# 개선된 Dataset 클래스\n",
        "class ImageDatasetV2(Dataset):\n",
        "    def __init__(self, data_dir, mapping_file_path, class_names, transform=None, max_size=(800, 800)):\n",
        "        self.data_dir = data_dir\n",
        "        self.class_names = class_names\n",
        "        self.transform = transform\n",
        "        self.max_size = max_size\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_names)}\n",
        "\n",
        "        with open(mapping_file_path, 'r', encoding='utf-8') as f:\n",
        "            self.filename_to_label = json.load(f)\n",
        "\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self._load_from_mapping()\n",
        "\n",
        "    def _load_from_mapping(self):\n",
        "        valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
        "\n",
        "        for file_name in os.listdir(self.data_dir):\n",
        "            if file_name.lower().endswith(valid_extensions):\n",
        "                if file_name in self.filename_to_label:\n",
        "                    label = self.filename_to_label[file_name]\n",
        "                    if label in self.class_to_idx:\n",
        "                        image_path = os.path.join(self.data_dir, file_name)\n",
        "                        self.image_paths.append(image_path)\n",
        "                        self.labels.append(self.class_to_idx[label])\n",
        "\n",
        "        print(f\"로드된 이미지: {len(self.image_paths)}개\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "            # 큰 이미지 자동 리사이즈 (성능 향상)\n",
        "            if image.size[0] * image.size[1] > 5000000:  # 5MP 이상\n",
        "                image.thumbnail(self.max_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"이미지 오류: {image_path}\")\n",
        "            image = Image.new('RGB', (224, 224), color='white')\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# 과적합 방지 강화 데이터 로더\n",
        "def create_robust_loaders(class_names):\n",
        "    \"\"\"과적합 방지를 위한 강화된 데이터 로더\"\"\"\n",
        "\n",
        "    # 매우 강한 데이터 증강\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomCrop((224, 224), padding=32, padding_mode='reflect'),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.RandomVerticalFlip(0.3),\n",
        "        transforms.RandomRotation(25),\n",
        "        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.25),\n",
        "        transforms.RandomGrayscale(p=0.15),\n",
        "        transforms.RandomPerspective(distortion_scale=0.3, p=0.4),\n",
        "        transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        transforms.RandomErasing(p=0.25, scale=(0.02, 0.33), ratio=(0.3, 3.3))\n",
        "    ])\n",
        "\n",
        "    valid_test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    loaders = {}\n",
        "\n",
        "    for split in ['train', 'valid', 'test']:\n",
        "        split_dir = os.path.join(ConfigV2.dataset_dir, split)\n",
        "        mapping_file_path = f'/content/{split}_mapping.json'\n",
        "\n",
        "        if os.path.exists(split_dir) and os.path.exists(mapping_file_path):\n",
        "            dataset = ImageDatasetV2(\n",
        "                data_dir=split_dir,\n",
        "                mapping_file_path=mapping_file_path,\n",
        "                class_names=class_names,\n",
        "                transform=train_transform if split == 'train' else valid_test_transform\n",
        "            )\n",
        "\n",
        "            if len(dataset) > 0:\n",
        "                loaders[split] = DataLoader(\n",
        "                    dataset,\n",
        "                    batch_size=ConfigV2.batch_size,\n",
        "                    shuffle=(split == 'train'),\n",
        "                    num_workers=ConfigV2.num_workers,\n",
        "                    pin_memory=True if torch.cuda.is_available() else False,\n",
        "                    drop_last=(split == 'train')\n",
        "                )\n",
        "                print(f\"{split}: {len(dataset)} samples\")\n",
        "\n",
        "    return loaders.get('train'), loaders.get('valid'), loaders.get('test')\n",
        "\n",
        "# 과적합 방지 모델 - 더 보수적\n",
        "def create_anti_overfit_model(num_classes):\n",
        "    \"\"\"과적합 방지에 특화된 모델\"\"\"\n",
        "    print(f\"과적합 방지 모델 생성 (클래스: {num_classes})\")\n",
        "\n",
        "    model = models.resnet50(pretrained=True)\n",
        "\n",
        "    # 대부분의 레이어 고정 (과적합 방지)\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'layer4' not in name and 'fc' not in name:  # layer4와 fc만 학습\n",
        "            param.requires_grad = False\n",
        "        else:\n",
        "            param.requires_grad = True\n",
        "\n",
        "    # 매우 보수적인 분류기\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(ConfigV2.dropout_rate),\n",
        "        nn.Linear(model.fc.in_features, 256),\n",
        "        nn.BatchNorm1d(256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(256, 128),\n",
        "        nn.BatchNorm1d(128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(128, num_classes)\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Early Stopping 클래스\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.best_weights = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(model)\n",
        "        elif val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            self.save_checkpoint(model)\n",
        "        else:\n",
        "            self.counter += 1\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            if self.restore_best_weights:\n",
        "                model.load_state_dict(self.best_weights)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        self.best_weights = copy.deepcopy(model.state_dict())\n",
        "\n",
        "# 체크포인트 저장 함수\n",
        "def save_checkpoint_v2(model, optimizer, scheduler, epoch, train_losses, train_accs,\n",
        "                      valid_losses, valid_accs, best_valid_acc):\n",
        "    \"\"\"V2 체크포인트 저장\"\"\"\n",
        "    os.makedirs(ConfigV2.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'train_losses': train_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'valid_losses': valid_losses,\n",
        "        'valid_accs': valid_accs,\n",
        "        'best_valid_acc': best_valid_acc,\n",
        "        'config': {\n",
        "            'lr': ConfigV2.lr,\n",
        "            'batch_size': ConfigV2.batch_size,\n",
        "            'dropout_rate': ConfigV2.dropout_rate,\n",
        "            'weight_decay': ConfigV2.weight_decay\n",
        "        },\n",
        "        'timestamp': ConfigV2.timestamp\n",
        "    }\n",
        "\n",
        "    checkpoint_path = os.path.join(ConfigV2.checkpoint_dir, f'checkpoint_epoch_{epoch}_{ConfigV2.timestamp}.pth')\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "    latest_path = os.path.join(ConfigV2.checkpoint_dir, f'latest_checkpoint_{ConfigV2.timestamp}.pth')\n",
        "    torch.save(checkpoint, latest_path)\n",
        "\n",
        "    print(f'✓ 체크포인트 저장: epoch {epoch+1}')\n",
        "    return checkpoint_path\n",
        "\n",
        "# 개선된 학습 함수\n",
        "def train_anti_overfit_model(model, train_loader, valid_loader, num_classes):\n",
        "    \"\"\"과적합 방지에 특화된 학습 함수\"\"\"\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # L2 정규화가 추가된 옵티마이저\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Label smoothing 추가\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=ConfigV2.lr, weight_decay=ConfigV2.weight_decay)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping = EarlyStopping(patience=ConfigV2.patience, min_delta=0.001)\n",
        "\n",
        "    # 기록\n",
        "    train_losses, train_accs = [], []\n",
        "    valid_losses, valid_accs = [], []\n",
        "    best_valid_acc = 0.0\n",
        "\n",
        "    print(f\"=== 과적합 방지 학습 시작 ===\")\n",
        "    print(f\"학습률: {ConfigV2.lr}, 배치크기: {ConfigV2.batch_size}\")\n",
        "    print(f\"Weight Decay: {ConfigV2.weight_decay}, Dropout: {ConfigV2.dropout_rate}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(ConfigV2.num_epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(f'\\nEpoch {epoch+1}/{ConfigV2.num_epochs}')\n",
        "        print('-' * 60)\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping (과적합 방지)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # 메모리 정리\n",
        "            if batch_idx % 50 == 0 and torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_train_acc = running_corrects.double() / len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "        if valid_loader is not None:\n",
        "            model.eval()\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in valid_loader:\n",
        "                    inputs = inputs.to(device, non_blocking=True)\n",
        "                    labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_valid_loss = running_loss / len(valid_loader.dataset)\n",
        "            epoch_valid_acc = running_corrects.double() / len(valid_loader.dataset)\n",
        "\n",
        "            # 스케줄러 업데이트\n",
        "            scheduler.step(epoch_valid_loss)\n",
        "\n",
        "            # 결과 출력\n",
        "            epoch_time = time.time() - epoch_start\n",
        "            overfitting = epoch_train_acc - epoch_valid_acc\n",
        "\n",
        "            print(f'Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f}')\n",
        "            print(f'Valid Loss: {epoch_valid_loss:.4f} Acc: {epoch_valid_acc:.4f}')\n",
        "            print(f'과적합 정도: {overfitting:.4f} | 시간: {epoch_time:.1f}초')\n",
        "            print(f'현재 학습률: {optimizer.param_groups[0][\"lr\"]:.2e}')\n",
        "\n",
        "            # 기록 저장\n",
        "            train_losses.append(epoch_train_loss)\n",
        "            train_accs.append(epoch_train_acc.cpu().numpy())\n",
        "            valid_losses.append(epoch_valid_loss)\n",
        "            valid_accs.append(epoch_valid_acc.cpu().numpy())\n",
        "\n",
        "            # Best model 업데이트\n",
        "            if epoch_valid_acc > best_valid_acc:\n",
        "                best_valid_acc = epoch_valid_acc\n",
        "                print(f'★ 새로운 최고 성능! Valid Acc: {best_valid_acc:.4f}')\n",
        "\n",
        "            # Early stopping 체크\n",
        "            if early_stopping(epoch_valid_loss, model):\n",
        "                print(f\"\\n⏹ Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "            # 체크포인트 저장\n",
        "            if (epoch + 1) % ConfigV2.save_checkpoint_every == 0:\n",
        "                save_checkpoint_v2(model, optimizer, scheduler, epoch,\n",
        "                                  train_losses, train_accs, valid_losses, valid_accs, best_valid_acc)\n",
        "\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "\n",
        "    # 최종 모델 저장\n",
        "    os.makedirs(ConfigV2.model_dir, exist_ok=True)\n",
        "    model_path = os.path.join(ConfigV2.model_dir, f'best_model_{ConfigV2.timestamp}.pth')\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f'\\n=== 학습 완료 ===')\n",
        "    print(f'최고 검증 정확도: {best_valid_acc:.4f}')\n",
        "    print(f'학습 시간: {training_time/60:.1f}분')\n",
        "    print(f'모델 저장: {model_path}')\n",
        "\n",
        "    return model, train_losses, train_accs, valid_losses, valid_accs\n",
        "\n",
        "# 클래스 이름\n",
        "class_names = [\n",
        "    '고정 불량_불량품', '고정 불량_양품', '고정핀 불량_불량품', '고정핀 불량_양품',\n",
        "    '단차_불량품', '단차_양품', '스크래치_불량품', '스크래치_양품',\n",
        "    '실링 불량_불량품', '실링 불량_양품', '연계 불량_불량품', '연계 불량_양품',\n",
        "    '외관 손상_불량품', '외관 손상_양품', '유격 불량_불량품', '유격 불량_양품',\n",
        "    '장착 불량_불량품', '장착 불량_양품', '체결 불량_불량품', '체결 불량_양품',\n",
        "    '헤밍 불량_불량품', '헤밍 불량_양품', '홀 변형_불량품', '홀 변형_양품'\n",
        "]\n",
        "\n",
        "print(\"=== 새 실험 준비 완료 ===\")\n",
        "print(\"다음 단계:\")\n",
        "print(\"1. train_loader, valid_loader, test_loader = create_robust_loaders(class_names)\")\n",
        "print(\"2. model = create_anti_overfit_model(len(class_names))\")\n",
        "print(\"3. trained_model, losses, accs, val_losses, val_accs = train_anti_overfit_model(model, train_loader, valid_loader, len(class_names))\")\n",
        "print(\"\")\n",
        "print(\"주요 개선사항:\")\n",
        "print(\"- 새 폴더로 완전히 새 시작\")\n",
        "print(\"- 매우 강한 정규화 (Dropout 0.7, Weight Decay, Label Smoothing)\")\n",
        "print(\"- 강화된 데이터 증강\")\n",
        "print(\"- Early Stopping\")\n",
        "print(\"- 대부분 레이어 고정 (layer4만 학습)\")\n",
        "print(\"- 작은 배치 크기 (12)\")\n",
        "print(\"- 낮은 학습률 (3e-5)\")\n",
        "\n",
        "train_loader, valid_loader, test_loader = create_robust_loaders(class_names)\n",
        "model = create_anti_overfit_model(len(class_names))\n",
        "trained_model, *_ = train_anti_overfit_model(model, train_loader, valid_loader, len(class_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lDHAfHzw9VI"
      },
      "source": [
        "## 고도화 2) 과적합 방지 장치 + 좀 더 좁은 간격으로 중간 모델 저장\n",
        "- 3 에포크마다 저장\n",
        "- checkpoint v2 폴더에 모델 저장 + 해당 폴더에 저장된 모델 불러와서 학습 이어서 진행 하는 코드\n",
        "- 의장공정/데이터/image_classification/ 위치에 3개의 _mapping.json 업로드 해둠\n",
        "\n",
        "\n",
        " -> 해당 json 파일을 코랩 로컬에 업로드 후 아래의 훈련 셀 실행해야함 (+ 구글 드라이브 마운트 코드도 실행해야함)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-BZHzvMeskr"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "import datetime\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# PIL 설정\n",
        "warnings.filterwarnings(\"ignore\", category=Image.DecompressionBombWarning)\n",
        "Image.MAX_IMAGE_PIXELS = None\n",
        "\n",
        "# GPU 설정\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    print(\"CPU 모드\")\n",
        "\n",
        "\n",
        "# 개선된 Dataset 클래스\n",
        "class ImageDatasetV2(Dataset):\n",
        "    def __init__(self, data_dir, mapping_file_path, class_names, transform=None, max_size=(800, 800)):\n",
        "        self.data_dir = data_dir\n",
        "        self.class_names = class_names\n",
        "        self.transform = transform\n",
        "        self.max_size = max_size\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_names)}\n",
        "\n",
        "        with open(mapping_file_path, 'r', encoding='utf-8') as f:\n",
        "            self.filename_to_label = json.load(f)\n",
        "\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self._load_from_mapping()\n",
        "\n",
        "    def _load_from_mapping(self):\n",
        "        valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
        "\n",
        "        for file_name in os.listdir(self.data_dir):\n",
        "            if file_name.lower().endswith(valid_extensions):\n",
        "                if file_name in self.filename_to_label:\n",
        "                    label = self.filename_to_label[file_name]\n",
        "                    if label in self.class_to_idx:\n",
        "                        image_path = os.path.join(self.data_dir, file_name)\n",
        "                        self.image_paths.append(image_path)\n",
        "                        self.labels.append(self.class_to_idx[label])\n",
        "\n",
        "        print(f\"로드된 이미지: {len(self.image_paths)}개\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "            # 큰 이미지 자동 리사이즈 (성능 향상)\n",
        "            if image.size[0] * image.size[1] > 5000000:  # 5MP 이상\n",
        "                image.thumbnail(self.max_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"이미지 오류: {image_path}\")\n",
        "            image = Image.new('RGB', (224, 224), color='white')\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# 과적합 방지 강화 데이터 로더\n",
        "def create_robust_loaders(class_names):\n",
        "    \"\"\"과적합 방지를 위한 강화된 데이터 로더\"\"\"\n",
        "\n",
        "    # 매우 강한 데이터 증강\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomCrop((224, 224), padding=32, padding_mode='reflect'),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.RandomVerticalFlip(0.3),\n",
        "        transforms.RandomRotation(25),\n",
        "        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.25),\n",
        "        transforms.RandomGrayscale(p=0.15),\n",
        "        transforms.RandomPerspective(distortion_scale=0.3, p=0.4),\n",
        "        transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        transforms.RandomErasing(p=0.25, scale=(0.02, 0.33), ratio=(0.3, 3.3))\n",
        "    ])\n",
        "\n",
        "    valid_test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    loaders = {}\n",
        "\n",
        "    for split in ['train', 'valid', 'test']:\n",
        "        split_dir = os.path.join(ConfigV2.dataset_dir, split)\n",
        "        mapping_file_path = f'/content/{split}_mapping.json'\n",
        "\n",
        "        if os.path.exists(split_dir) and os.path.exists(mapping_file_path):\n",
        "            dataset = ImageDatasetV2(\n",
        "                data_dir=split_dir,\n",
        "                mapping_file_path=mapping_file_path,\n",
        "                class_names=class_names,\n",
        "                transform=train_transform if split == 'train' else valid_test_transform\n",
        "            )\n",
        "\n",
        "            if len(dataset) > 0:\n",
        "                loaders[split] = DataLoader(\n",
        "                    dataset,\n",
        "                    batch_size=ConfigV2.batch_size,\n",
        "                    shuffle=(split == 'train'),\n",
        "                    num_workers=ConfigV2.num_workers,\n",
        "                    pin_memory=True if torch.cuda.is_available() else False,\n",
        "                    drop_last=(split == 'train')\n",
        "                )\n",
        "                print(f\"{split}: {len(dataset)} samples\")\n",
        "\n",
        "    return loaders.get('train'), loaders.get('valid'), loaders.get('test')\n",
        "\n",
        "# 과적합 방지 모델 - 더 보수적\n",
        "def create_anti_overfit_model(num_classes):\n",
        "    \"\"\"과적합 방지에 특화된 모델\"\"\"\n",
        "    print(f\"과적합 방지 모델 생성 (클래스: {num_classes})\")\n",
        "\n",
        "    model = models.resnet50(pretrained=True)\n",
        "\n",
        "    # 대부분의 레이어 고정 (과적합 방지)\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'layer4' not in name and 'fc' not in name:  # layer4와 fc만 학습\n",
        "            param.requires_grad = False\n",
        "        else:\n",
        "            param.requires_grad = True\n",
        "\n",
        "    # 매우 보수적인 분류기\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(ConfigV2.dropout_rate),\n",
        "        nn.Linear(model.fc.in_features, 256),\n",
        "        nn.BatchNorm1d(256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(256, 128),\n",
        "        nn.BatchNorm1d(128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(128, num_classes)\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Early Stopping 클래스\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.best_weights = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(model)\n",
        "        elif val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            self.save_checkpoint(model)\n",
        "        else:\n",
        "            self.counter += 1\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            if self.restore_best_weights:\n",
        "                model.load_state_dict(self.best_weights)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        self.best_weights = copy.deepcopy(model.state_dict())\n",
        "\n",
        "# 타임스탬프 고정 (이전 실험과 동일하게)\n",
        "class ConfigV2:\n",
        "    # 새로운 경로 설정\n",
        "    work_dir = '/content/drive/MyDrive/의장공정/데이터/image_classification'\n",
        "    dataset_dir = os.path.join(work_dir, 'dataset')\n",
        "\n",
        "    # 새 폴더들 - v2로 구분\n",
        "    model_dir = os.path.join(work_dir, 'models_v2')\n",
        "    result_dir = os.path.join(work_dir, 'results_v2')\n",
        "    checkpoint_dir = os.path.join(work_dir, 'checkpoints_v2')\n",
        "\n",
        "    # 과적합 방지를 위한 학습 설정\n",
        "    IMG_SIZE = (224, 224)\n",
        "    batch_size = 12  # 더 작게\n",
        "    num_workers = 0  # 멀티프로세싱 비활성화\n",
        "    lr = 3e-5  # 더 낮은 학습률\n",
        "    num_epochs = 40  # 더 적은 에포크\n",
        "\n",
        "    # 정규화 설정\n",
        "    weight_decay = 1e-4  # L2 정규화\n",
        "    dropout_rate = 0.7\n",
        "\n",
        "    # 체크포인트 설정\n",
        "    save_checkpoint_every = 3\n",
        "\n",
        "    # 이전 실험의 타임스탬프로 고정\n",
        "    timestamp = '250804_2305'  # 고정!\n",
        "\n",
        "    # Early stopping 설정\n",
        "    patience = 8  # 8 에포크 동안 개선 없으면 중단\n",
        "    min_delta = 0.001  # 최소 개선 임계값\n",
        "\n",
        "print(f\"이전 실험 재개 - 타임스탬프: {ConfigV2.timestamp}\")\n",
        "print(f\"저장 폴더: models_v2, results_v2, checkpoints_v2\")\n",
        "\n",
        "# 체크포인트 찾기 및 로드 함수들\n",
        "def find_latest_checkpoint_v2():\n",
        "    \"\"\"이전 실험의 최신 체크포인트 찾기\"\"\"\n",
        "    checkpoint_dir = ConfigV2.checkpoint_dir\n",
        "    timestamp = ConfigV2.timestamp\n",
        "\n",
        "    print(f\"체크포인트 디렉토리 확인: {checkpoint_dir}\")\n",
        "\n",
        "    # 최신 체크포인트 경로 먼저 확인\n",
        "    latest_path = os.path.join(checkpoint_dir, f'latest_checkpoint_{timestamp}.pth')\n",
        "    if os.path.exists(latest_path):\n",
        "        print(f\"✓ 최신 체크포인트 발견: {latest_path}\")\n",
        "        return latest_path\n",
        "\n",
        "    # 개별 체크포인트들 찾기\n",
        "    checkpoints = []\n",
        "    if os.path.exists(checkpoint_dir):\n",
        "        for file in os.listdir(checkpoint_dir):\n",
        "            if file.startswith('checkpoint_epoch_') and timestamp in file:\n",
        "                try:\n",
        "                    # 파일명에서 에포크 번호 추출\n",
        "                    parts = file.split('_')\n",
        "                    epoch_num = int(parts[2])\n",
        "                    checkpoints.append((epoch_num, os.path.join(checkpoint_dir, file)))\n",
        "                    print(f\"  발견된 체크포인트: {file} (에포크 {epoch_num})\")\n",
        "                except (IndexError, ValueError):\n",
        "                    continue\n",
        "\n",
        "    if checkpoints:\n",
        "        checkpoints.sort(reverse=True)  # 에포크 번호 내림차순 정렬\n",
        "        latest_checkpoint = checkpoints[0][1]\n",
        "        print(f\"✓ 가장 최근 체크포인트: {latest_checkpoint} (에포크 {checkpoints[0][0]})\")\n",
        "        return latest_checkpoint\n",
        "\n",
        "    print(\"✗ 체크포인트를 찾을 수 없습니다.\")\n",
        "    return None\n",
        "\n",
        "def load_checkpoint_v2(checkpoint_path, model, optimizer, scheduler):\n",
        "    \"\"\"체크포인트 로드\"\"\"\n",
        "    print(f\"체크포인트 로드 중: {checkpoint_path}\")\n",
        "\n",
        "    try:\n",
        "        checkpoint = torch.load(checkpoint_path, weights_only=False, map_location='cpu')\n",
        "\n",
        "        # 모델 상태 로드\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "        # 학습 기록 로드\n",
        "        start_epoch = checkpoint['epoch'] + 1  # 다음 에포크부터 시작\n",
        "        train_losses = checkpoint['train_losses']\n",
        "        train_accs = checkpoint['train_accs']\n",
        "        valid_losses = checkpoint['valid_losses']\n",
        "        valid_accs = checkpoint['valid_accs']\n",
        "        best_valid_acc = checkpoint['best_valid_acc']\n",
        "\n",
        "        print(f\"✓ 체크포인트 로드 완료!\")\n",
        "        print(f\"  - 재개 에포크: {start_epoch}\")\n",
        "        print(f\"  - 최고 검증 정확도: {best_valid_acc:.4f}\")\n",
        "        print(f\"  - 기존 학습 기록: {len(train_losses)}개 에포크\")\n",
        "\n",
        "        return start_epoch, train_losses, train_accs, valid_losses, valid_accs, best_valid_acc\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ 체크포인트 로드 실패: {e}\")\n",
        "        return None\n",
        "\n",
        "# 수정된 학습 함수 (체크포인트 재개 기능 추가)\n",
        "def train_anti_overfit_model_resume(model, train_loader, valid_loader, num_classes):\n",
        "    \"\"\"체크포인트에서 재개 가능한 학습 함수\"\"\"\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # 옵티마이저와 스케줄러 설정\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=ConfigV2.lr, weight_decay=ConfigV2.weight_decay)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "    # 체크포인트에서 재개 시도\n",
        "    latest_checkpoint = find_latest_checkpoint_v2()\n",
        "\n",
        "    if latest_checkpoint:\n",
        "        resume_data = load_checkpoint_v2(latest_checkpoint, model, optimizer, scheduler)\n",
        "        if resume_data:\n",
        "            start_epoch, train_losses, train_accs, valid_losses, valid_accs, best_valid_acc = resume_data\n",
        "            print(f\"\\n🔄 에포크 {start_epoch}부터 학습 재개!\")\n",
        "        else:\n",
        "            print(\"\\n❌ 체크포인트 로드 실패 - 처음부터 시작\")\n",
        "            start_epoch = 0\n",
        "            train_losses, train_accs = [], []\n",
        "            valid_losses, valid_accs = [], []\n",
        "            best_valid_acc = 0.0\n",
        "    else:\n",
        "        print(\"\\n🆕 체크포인트 없음 - 처음부터 시작\")\n",
        "        start_epoch = 0\n",
        "        train_losses, train_accs = [], []\n",
        "        valid_losses, valid_accs = [], []\n",
        "        best_valid_acc = 0.0\n",
        "\n",
        "    # Early stopping 초기화\n",
        "    early_stopping = EarlyStopping(patience=ConfigV2.patience, min_delta=0.001)\n",
        "\n",
        "    print(f\"\\n=== 학습 시작/재개 ===\")\n",
        "    print(f\"시작 에포크: {start_epoch + 1}/{ConfigV2.num_epochs}\")\n",
        "    print(f\"학습률: {ConfigV2.lr}, 배치크기: {ConfigV2.batch_size}\")\n",
        "    print(f\"Weight Decay: {ConfigV2.weight_decay}, Dropout: {ConfigV2.dropout_rate}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(start_epoch, ConfigV2.num_epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(f'\\nEpoch {epoch+1}/{ConfigV2.num_epochs}')\n",
        "        print('-' * 60)\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping (과적합 방지)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # 메모리 정리\n",
        "            if batch_idx % 50 == 0 and torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_train_acc = running_corrects.double() / len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "        if valid_loader is not None:\n",
        "            model.eval()\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in valid_loader:\n",
        "                    inputs = inputs.to(device, non_blocking=True)\n",
        "                    labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_valid_loss = running_loss / len(valid_loader.dataset)\n",
        "            epoch_valid_acc = running_corrects.double() / len(valid_loader.dataset)\n",
        "\n",
        "            # 스케줄러 업데이트\n",
        "            scheduler.step(epoch_valid_loss)\n",
        "\n",
        "            # 결과 출력\n",
        "            epoch_time = time.time() - epoch_start\n",
        "            overfitting = epoch_train_acc - epoch_valid_acc\n",
        "\n",
        "            print(f'Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f}')\n",
        "            print(f'Valid Loss: {epoch_valid_loss:.4f} Acc: {epoch_valid_acc:.4f}')\n",
        "            print(f'과적합 정도: {overfitting:.4f} | 시간: {epoch_time:.1f}초')\n",
        "            print(f'현재 학습률: {optimizer.param_groups[0][\"lr\"]:.2e}')\n",
        "\n",
        "            # 기록 저장\n",
        "            train_losses.append(epoch_train_loss)\n",
        "            train_accs.append(epoch_train_acc.cpu().numpy())\n",
        "            valid_losses.append(epoch_valid_loss)\n",
        "            valid_accs.append(epoch_valid_acc.cpu().numpy())\n",
        "\n",
        "            # Best model 업데이트\n",
        "            if epoch_valid_acc > best_valid_acc:\n",
        "                best_valid_acc = epoch_valid_acc\n",
        "                print(f'★ 새로운 최고 성능! Valid Acc: {best_valid_acc:.4f}')\n",
        "\n",
        "            # Early stopping 체크\n",
        "            if early_stopping(epoch_valid_loss, model):\n",
        "                print(f\"\\n⏹ Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "            # 체크포인트 저장\n",
        "            if (epoch + 1) % ConfigV2.save_checkpoint_every == 0:\n",
        "                save_checkpoint_v2(model, optimizer, scheduler, epoch,\n",
        "                                  train_losses, train_accs, valid_losses, valid_accs, best_valid_acc)\n",
        "\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "\n",
        "    # 최종 모델 저장\n",
        "    os.makedirs(ConfigV2.model_dir, exist_ok=True)\n",
        "    model_path = os.path.join(ConfigV2.model_dir, f'best_model_{ConfigV2.timestamp}.pth')\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f'\\n=== 학습 완료 ===')\n",
        "    print(f'최고 검증 정확도: {best_valid_acc:.4f}')\n",
        "    print(f'학습 시간: {training_time/60:.1f}분')\n",
        "    print(f'모델 저장: {model_path}')\n",
        "\n",
        "    return model, train_losses, train_accs, valid_losses, valid_accs\n",
        "\n",
        "def save_checkpoint_v2(model, optimizer, scheduler, epoch, train_losses, train_accs,\n",
        "                      valid_losses, valid_accs, best_valid_acc):\n",
        "    \"\"\"V2 체크포인트 저장\"\"\"\n",
        "    os.makedirs(ConfigV2.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'train_losses': train_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'valid_losses': valid_losses,\n",
        "        'valid_accs': valid_accs,\n",
        "        'best_valid_acc': best_valid_acc,\n",
        "        'config': {\n",
        "            'lr': ConfigV2.lr,\n",
        "            'batch_size': ConfigV2.batch_size,\n",
        "            'dropout_rate': ConfigV2.dropout_rate,\n",
        "            'weight_decay': ConfigV2.weight_decay\n",
        "        },\n",
        "        'timestamp': ConfigV2.timestamp\n",
        "    }\n",
        "\n",
        "    checkpoint_path = os.path.join(ConfigV2.checkpoint_dir, f'checkpoint_epoch_{epoch}_{ConfigV2.timestamp}.pth')\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "    latest_path = os.path.join(ConfigV2.checkpoint_dir, f'latest_checkpoint_{ConfigV2.timestamp}.pth')\n",
        "    torch.save(checkpoint, latest_path)\n",
        "\n",
        "    print(f'✓ 체크포인트 저장: epoch {epoch+1}')\n",
        "    return checkpoint_path\n",
        "\n",
        "# 실행 코드\n",
        "print(\"=== 체크포인트에서 학습 재개 준비 완료 ===\")\n",
        "print(\"실행 순서:\")\n",
        "print(\"1. train_loader, valid_loader, test_loader = create_robust_loaders(class_names)\")\n",
        "print(\"2. model = create_anti_overfit_model(len(class_names))\")\n",
        "print(\"3. trained_model, *_ = train_anti_overfit_model_resume(model, train_loader, valid_loader, len(class_names))\")\n",
        "\n",
        "\n",
        "# 클래스 이름\n",
        "class_names = [\n",
        "    '고정 불량_불량품', '고정 불량_양품', '고정핀 불량_불량품', '고정핀 불량_양품',\n",
        "    '단차_불량품', '단차_양품', '스크래치_불량품', '스크래치_양품',\n",
        "    '실링 불량_불량품', '실링 불량_양품', '연계 불량_불량품', '연계 불량_양품',\n",
        "    '외관 손상_불량품', '외관 손상_양품', '유격 불량_불량품', '유격 불량_양품',\n",
        "    '장착 불량_불량품', '장착 불량_양품', '체결 불량_불량품', '체결 불량_양품',\n",
        "    '헤밍 불량_불량품', '헤밍 불량_양품', '홀 변형_불량품', '홀 변형_양품'\n",
        "]\n",
        "\n",
        "# 자동 실행 (기존 코드와 동일한 방식)\n",
        "train_loader, valid_loader, test_loader = create_robust_loaders(class_names)\n",
        "model = create_anti_overfit_model(len(class_names))\n",
        "trained_model, *_ = train_anti_overfit_model_resume(model, train_loader, valid_loader, len(class_names))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
