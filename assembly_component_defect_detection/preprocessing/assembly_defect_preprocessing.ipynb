{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 드라이브 마운트"
      ],
      "metadata": {
        "id": "hqos_Oyb0fp6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "c5cOJ-3Si0yk",
        "outputId": "e1ae8a42-b6a1-4a18-f3b7-81bc4981561c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라이브러리 임포트"
      ],
      "metadata": {
        "id": "aZprTo_j0kt5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z--c93OvcWVG"
      },
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 import\n",
        "import os\n",
        "import json\n",
        "import datetime\n",
        "import shutil\n",
        "import time\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT3yZjfkboqx",
        "outputId": "18654caf-11d6-4790-e268-dfaa2a1bd527"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 이미지 분류 학습 시작 ===\n",
            "데이터 로딩 및 분석 중...\n",
            "원천데이터 이미지 수: 21553\n",
            "라벨링데이터 파일 수: 21553\n",
            "이미지-라벨 매칭 중...\n",
            "라벨링 데이터 구조 분석 중... (샘플 5개)\n",
            "\n",
            "--- 샘플 1: 204_101_20_1bace470-0fad-4b08-a0f2-f399063a9f6f.json ---\n",
            "JSON 구조:\n",
            "{\n",
            "  \"info\": {\n",
            "    \"contributor\": \"미래아이티컨소시엄\",\n",
            "    \"date_created\": \"2021-12-02 08:54:24.241591\",\n",
            "    \"name\": \"부품 품질 검사 영상 데이터(자동차)\",\n",
            "    \"description\": \"부품 품질 검사 영상 데이터(자동차)\",\n",
            "    \"version\": \"1.0\",\n",
            "    \"url\": \"miraeit.net/\"\n",
            "  },\n",
            "  \"images\": [\n",
            "    {\n",
            "      \"license\": 1,\n",
            "      \"file_name\": \"204_101_20_1bace470-0fad-4b08-a0f2-f399063a9f6f.jpg\",\n",
            "      \"width\": 4000,\n",
            "      \"date_captured\": \"2021-11-25 13:57:21\",\n",
            "      \"id\": 1,\n",
            "      \"height\": 3000\n",
            "    }\n",
            "  ],\n",
            "  \"licenses\": [\n",
            "    {\n",
            "      \"name\": \"CC BY-N...\n",
            "주요 키들: ['info', 'images', 'licenses', 'categories', 'annotations']\n",
            "\n",
            "--- 샘플 2: 204_101_20_1bf6a4a7-ef91-4efb-aee4-965af8326c49.json ---\n",
            "JSON 구조:\n",
            "{\n",
            "  \"info\": {\n",
            "    \"contributor\": \"미래아이티컨소시엄\",\n",
            "    \"date_created\": \"2021-12-24 13:52:22.338968\",\n",
            "    \"name\": \"부품 품질 검사 영상 데이터(자동차)\",\n",
            "    \"description\": \"부품 품질 검사 영상 데이터(자동차)\",\n",
            "    \"version\": \"1.0\",\n",
            "    \"url\": \"miraeit.net/\"\n",
            "  },\n",
            "  \"images\": [\n",
            "    {\n",
            "      \"license\": 1,\n",
            "      \"file_name\": \"204_101_20_1bf6a4a7-ef91-4efb-aee4-965af8326c49.JPG\",\n",
            "      \"width\": 4032,\n",
            "      \"date_captured\": \"2021-12-15 21:23:29\",\n",
            "      \"id\": 1,\n",
            "      \"height\": 2268\n",
            "    }\n",
            "  ],\n",
            "  \"licenses\": [\n",
            "    {\n",
            "      \"name\": \"CC BY-N...\n",
            "주요 키들: ['info', 'images', 'licenses', 'categories', 'annotations']\n",
            "\n",
            "--- 샘플 3: 204_101_20_1c65599f-3bc1-4297-9179-fc6f5fb8c0ea.json ---\n",
            "JSON 구조:\n",
            "{\n",
            "  \"info\": {\n",
            "    \"contributor\": \"미래아이티컨소시엄\",\n",
            "    \"date_created\": \"2021-12-24 15:34:33.045516\",\n",
            "    \"name\": \"부품 품질 검사 영상 데이터(자동차)\",\n",
            "    \"description\": \"부품 품질 검사 영상 데이터(자동차)\",\n",
            "    \"version\": \"1.0\",\n",
            "    \"url\": \"miraeit.net/\"\n",
            "  },\n",
            "  \"images\": [\n",
            "    {\n",
            "      \"license\": 1,\n",
            "      \"file_name\": \"204_101_20_1c65599f-3bc1-4297-9179-fc6f5fb8c0ea.jpg\",\n",
            "      \"width\": 1920,\n",
            "      \"date_captured\": \"2021-12-14 12:12:41\",\n",
            "      \"id\": 1,\n",
            "      \"height\": 1080\n",
            "    }\n",
            "  ],\n",
            "  \"licenses\": [\n",
            "    {\n",
            "      \"name\": \"CC BY-N...\n",
            "주요 키들: ['info', 'images', 'licenses', 'categories', 'annotations']\n",
            "\n",
            "--- 샘플 4: 204_101_20_1cd1e82c-0736-4442-b377-aafd5091528c.json ---\n",
            "JSON 구조:\n",
            "{\n",
            "  \"info\": {\n",
            "    \"contributor\": \"미래아이티컨소시엄\",\n",
            "    \"date_created\": \"2021-12-20 15:54:00.28187\",\n",
            "    \"name\": \"부품 품질 검사 영상 데이터(자동차)\",\n",
            "    \"description\": \"부품 품질 검사 영상 데이터(자동차)\",\n",
            "    \"version\": \"1.0\",\n",
            "    \"url\": \"miraeit.net/\"\n",
            "  },\n",
            "  \"images\": [\n",
            "    {\n",
            "      \"license\": 1,\n",
            "      \"file_name\": \"204_101_20_1cd1e82c-0736-4442-b377-aafd5091528c.jpg\",\n",
            "      \"width\": 4224,\n",
            "      \"date_captured\": \"2021-12-14 11:43:57\",\n",
            "      \"id\": 1,\n",
            "      \"height\": 2376\n",
            "    }\n",
            "  ],\n",
            "  \"licenses\": [\n",
            "    {\n",
            "      \"name\": \"CC BY-NC...\n",
            "주요 키들: ['info', 'images', 'licenses', 'categories', 'annotations']\n",
            "\n",
            "--- 샘플 5: 204_101_20_1cf0b04f-72b3-4b7d-8702-14eddef8996b.json ---\n",
            "JSON 구조:\n",
            "{\n",
            "  \"info\": {\n",
            "    \"contributor\": \"미래아이티컨소시엄\",\n",
            "    \"date_created\": \"2021-12-25 16:48:45.309279\",\n",
            "    \"name\": \"부품 품질 검사 영상 데이터(자동차)\",\n",
            "    \"description\": \"부품 품질 검사 영상 데이터(자동차)\",\n",
            "    \"version\": \"1.0\",\n",
            "    \"url\": \"miraeit.net/\"\n",
            "  },\n",
            "  \"images\": [\n",
            "    {\n",
            "      \"license\": 1,\n",
            "      \"file_name\": \"204_101_20_1cf0b04f-72b3-4b7d-8702-14eddef8996b.jpg\",\n",
            "      \"width\": 4624,\n",
            "      \"date_captured\": \"2021-12-16 14:56:20\",\n",
            "      \"id\": 1,\n",
            "      \"height\": 2084\n",
            "    }\n",
            "  ],\n",
            "  \"licenses\": [\n",
            "    {\n",
            "      \"name\": \"CC BY-N...\n",
            "주요 키들: ['info', 'images', 'licenses', 'categories', 'annotations']\n",
            "\n",
            "==================================================\n",
            "라벨 딕셔너리 생성 완료: 21553개\n",
            "배치 처리 중: 0-1000 (0.0%)\n",
            "  이 배치 결과: 1000개 매칭\n",
            "배치 처리 중: 1000-2000 (4.6%)\n",
            "  이 배치 결과: 1000개 매칭\n",
            "배치 처리 중: 2000-3000 (9.3%)\n",
            "  이 배치 결과: 1000개 매칭\n",
            "배치 처리 중: 3000-4000 (13.9%)\n",
            "  이 배치 결과: 1000개 매칭\n",
            "배치 처리 중: 4000-5000 (18.6%)\n",
            "  이 배치 결과: 1000개 매칭\n",
            "배치 처리 중: 5000-6000 (23.2%)\n",
            "  이 배치 결과: 1000개 매칭\n",
            "배치 처리 중: 6000-7000 (27.8%)\n",
            "  이 배치 결과: 1000개 매칭\n",
            "배치 처리 중: 7000-8000 (32.5%)\n",
            "  이 배치 결과: 1000개 매칭\n",
            "배치 처리 중: 8000-9000 (37.1%)\n",
            "  이 배치 결과: 1000개 매칭\n",
            "배치 처리 중: 9000-10000 (41.8%)\n",
            "  이 배치 결과: 1000개 매칭\n",
            "배치 처리 중: 10000-11000 (46.4%)\n",
            "  이 배치 결과: 1000개 매칭\n",
            "배치 처리 중: 11000-12000 (51.0%)\n",
            "  이 배치 결과: 1000개 매칭\n",
            "배치 처리 중: 12000-13000 (55.7%)\n",
            "  이 배치 결과: 1000개 매칭\n",
            "배치 처리 중: 13000-14000 (60.3%)\n",
            "  이 배치 결과: 1000개 매칭\n",
            "배치 처리 중: 14000-15000 (65.0%)\n",
            "  이 배치 결과: 1000개 매칭\n",
            "배치 처리 중: 15000-16000 (69.6%)\n",
            "  이 배치 결과: 1000개 매칭\n",
            "배치 처리 중: 16000-17000 (74.2%)\n",
            "  이 배치 결과: 1000개 매칭\n",
            "배치 처리 중: 17000-18000 (78.9%)\n",
            "  이 배치 결과: 1000개 매칭\n",
            "배치 처리 중: 18000-19000 (83.5%)\n",
            "  이 배치 결과: 1000개 매칭\n",
            "배치 처리 중: 19000-20000 (88.2%)\n",
            "  이 배치 결과: 1000개 매칭\n",
            "배치 처리 중: 20000-21000 (92.8%)\n",
            "  이 배치 결과: 1000개 매칭\n",
            "배치 처리 중: 21000-21553 (97.4%)\n",
            "  이 배치 결과: 553개 매칭\n",
            "\n",
            "매칭 완료!\n",
            "매칭된 이미지-라벨 쌍: 21553\n",
            "발견된 클래스 수: 24\n",
            "\n",
            "=== 클래스별 데이터 개수 ===\n",
            "  고정 불량_불량품: 757개\n",
            "  고정 불량_양품: 603개\n",
            "  고정핀 불량_불량품: 651개\n",
            "  고정핀 불량_양품: 644개\n",
            "  단차_불량품: 2714개\n",
            "  단차_양품: 2718개\n",
            "  스크래치_불량품: 1752개\n",
            "  스크래치_양품: 895개\n",
            "  실링 불량_불량품: 423개\n",
            "  실링 불량_양품: 214개\n",
            "  연계 불량_불량품: 636개\n",
            "  연계 불량_양품: 618개\n",
            "  외관 손상_불량품: 1152개\n",
            "  외관 손상_양품: 1060개\n",
            "  유격 불량_불량품: 982개\n",
            "  유격 불량_양품: 630개\n",
            "  장착 불량_불량품: 645개\n",
            "  장착 불량_양품: 650개\n",
            "  체결 불량_불량품: 1337개\n",
            "  체결 불량_양품: 587개\n",
            "  헤밍 불량_불량품: 543개\n",
            "  헤밍 불량_양품: 506개\n",
            "  홀 변형_불량품: 401개\n",
            "  홀 변형_양품: 435개\n",
            "\n",
            "=== 카테고리별 데이터 개수 ===\n",
            "  고정 불량: 1360개\n",
            "  고정핀 불량: 1295개\n",
            "  단차: 5432개\n",
            "  스크래치: 2647개\n",
            "  실링 불량: 637개\n",
            "  연계 불량: 1254개\n",
            "  외관 손상: 2212개\n",
            "  유격 불량: 1612개\n",
            "  장착 불량: 1295개\n",
            "  체결 불량: 1924개\n",
            "  헤밍 불량: 1049개\n",
            "  홀 변형: 836개\n",
            "\n",
            "=== 품질별 데이터 개수 ===\n",
            "  불량품: 11993개\n",
            "  양품: 9560개\n",
            "데이터셋 분할 및 복사 중...\n",
            "클래스 고정 불량_불량품: train=529, valid=114, test=114\n",
            "클래스 고정 불량_양품: train=422, valid=90, test=91\n",
            "클래스 고정핀 불량_불량품: train=455, valid=98, test=98\n",
            "클래스 고정핀 불량_양품: train=450, valid=97, test=97\n",
            "클래스 단차_불량품: train=1899, valid=407, test=408\n",
            "클래스 단차_양품: train=1902, valid=408, test=408\n",
            "클래스 스크래치_불량품: train=1226, valid=263, test=263\n",
            "클래스 스크래치_양품: train=626, valid=134, test=135\n",
            "클래스 실링 불량_불량품: train=296, valid=63, test=64\n",
            "클래스 실링 불량_양품: train=149, valid=32, test=33\n",
            "클래스 연계 불량_불량품: train=445, valid=95, test=96\n",
            "클래스 연계 불량_양품: train=432, valid=93, test=93\n",
            "클래스 외관 손상_불량품: train=806, valid=173, test=173\n",
            "클래스 외관 손상_양품: train=742, valid=159, test=159\n",
            "클래스 유격 불량_불량품: train=687, valid=147, test=148\n",
            "클래스 유격 불량_양품: train=441, valid=94, test=95\n",
            "클래스 장착 불량_불량품: train=451, valid=97, test=97\n",
            "클래스 장착 불량_양품: train=455, valid=97, test=98\n",
            "클래스 체결 불량_불량품: train=935, valid=201, test=201\n",
            "클래스 체결 불량_양품: train=410, valid=88, test=89\n",
            "클래스 헤밍 불량_불량품: train=380, valid=81, test=82\n",
            "클래스 헤밍 불량_양품: train=354, valid=76, test=76\n",
            "클래스 홀 변형_불량품: train=280, valid=60, test=61\n",
            "클래스 홀 변형_양품: train=304, valid=65, test=66\n",
            "\n",
            "train 데이터 복사 중... (15076개)\n",
            "\n",
            "valid 데이터 복사 중... (3232개)\n",
            "\n",
            "test 데이터 복사 중... (3245개)\n",
            "훈련 데이터가 없습니다. 종료합니다.\n"
          ]
        }
      ],
      "source": [
        "# 이미지 분류 데이터 전처리 코드\n",
        "\n",
        "import os\n",
        "import json\n",
        "import datetime\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class Config:\n",
        "    drive_root = '/content/drive/MyDrive/의장공정/데이터/Validation'\n",
        "    original_data_path = os.path.join(drive_root, '원천데이터')\n",
        "    label_data_path = os.path.join(drive_root, '라벨링데이터')\n",
        "\n",
        "    # 작업 디렉토리 설정\n",
        "    work_dir = '/content/drive/MyDrive/의장공정/데이터/image_classification'\n",
        "    dataset_dir = os.path.join(work_dir, 'dataset')\n",
        "    model_dir = os.path.join(work_dir, 'models')\n",
        "    result_dir = os.path.join(work_dir, 'results')\n",
        "\n",
        "    # 데이터 분할 비율\n",
        "    train_ratio = 0.7\n",
        "    valid_ratio = 0.15\n",
        "    test_ratio = 0.15\n",
        "\n",
        "    # 현재 시간\n",
        "    now = datetime.datetime.now()\n",
        "    timestamp = now.strftime('%y%m%d_%H%M')\n",
        "\n",
        "def create_directories():\n",
        "    \"\"\"필요한 디렉토리들을 생성합니다.\"\"\"\n",
        "    dirs_to_create = [\n",
        "        Config.work_dir,\n",
        "        Config.dataset_dir,\n",
        "        Config.model_dir,\n",
        "        Config.result_dir,\n",
        "        os.path.join(Config.dataset_dir, 'train'),\n",
        "        os.path.join(Config.dataset_dir, 'valid'),\n",
        "        os.path.join(Config.dataset_dir, 'test')\n",
        "    ]\n",
        "\n",
        "    for dir_path in dirs_to_create:\n",
        "        if not os.path.exists(dir_path):\n",
        "            os.makedirs(dir_path)\n",
        "            print(f\"Created directory: {dir_path}\")\n",
        "\n",
        "def split_and_copy_dataset(paired_data, class_names):\n",
        "    \"\"\"데이터를 train/valid/test로 분할하고 복사합니다.\"\"\"\n",
        "    print(\"데이터셋 분할 및 복사 중...\")\n",
        "\n",
        "    # 클래스별로 데이터 분할\n",
        "    train_data = []\n",
        "    valid_data = []\n",
        "    test_data = []\n",
        "\n",
        "    # 클래스별로 분할\n",
        "    for class_name in class_names:\n",
        "        class_data = [item for item in paired_data if item['label'] == class_name]\n",
        "\n",
        "        if len(class_data) == 0:\n",
        "            continue\n",
        "\n",
        "        # 데이터가 너무 적으면 train에만 할당\n",
        "        if len(class_data) <= 2:\n",
        "            train_data.extend(class_data)\n",
        "            print(f\"클래스 {class_name}: train={len(class_data)}, valid=0, test=0 (데이터 부족)\")\n",
        "            continue\n",
        "\n",
        "        # train/valid/test 분할\n",
        "        train_class, temp = train_test_split(\n",
        "            class_data,\n",
        "            test_size=(Config.valid_ratio + Config.test_ratio),\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        if len(temp) > 1:\n",
        "            valid_class, test_class = train_test_split(\n",
        "                temp,\n",
        "                test_size=Config.test_ratio/(Config.valid_ratio + Config.test_ratio),\n",
        "                random_state=42\n",
        "            )\n",
        "        else:\n",
        "            valid_class = temp\n",
        "            test_class = []\n",
        "\n",
        "        train_data.extend(train_class)\n",
        "        valid_data.extend(valid_class)\n",
        "        test_data.extend(test_class)\n",
        "\n",
        "        print(f\"클래스 {class_name}: train={len(train_class)}, valid={len(valid_class)}, test={len(test_class)}\")\n",
        "\n",
        "    # 파일 복사\n",
        "    datasets = {\n",
        "        'train': train_data,\n",
        "        'valid': valid_data,\n",
        "        'test': test_data\n",
        "    }\n",
        "\n",
        "    for split_name, data_list in datasets.items():\n",
        "        split_dir = os.path.join(Config.dataset_dir, split_name)\n",
        "\n",
        "        print(f\"\\n{split_name} 데이터 복사 중... ({len(data_list)}개)\")\n",
        "        for item in data_list:\n",
        "            src_path = item['image_path']\n",
        "            dst_path = os.path.join(split_dir, item['image_name'])\n",
        "\n",
        "            try:\n",
        "                shutil.copy2(src_path, dst_path)\n",
        "            except Exception as e:\n",
        "                print(f\"파일 복사 오류: {e}\")\n",
        "\n",
        "    return len(train_data), len(valid_data), len(test_data)\n",
        "\n",
        "def create_mapping_files_v3():\n",
        "    \"\"\"전체 폴더를 탐색해서 JSON 파일 찾고 매핑 생성\"\"\"\n",
        "    dataset_dir = '/content/drive/MyDrive/의장공정/데이터/image_classification/dataset'\n",
        "    validation_dir = '/content/drive/MyDrive/의장공정/데이터/Validation'\n",
        "\n",
        "    print(\"올바른 JSON 경로로 매핑 파일 재생성 시작...\")\n",
        "\n",
        "    # 모든 JSON 파일 찾기\n",
        "    print(\"JSON 파일 탐색 중...\")\n",
        "    all_json_files = []\n",
        "    for root, dirs, files in os.walk(validation_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.json'):\n",
        "                all_json_files.append(os.path.join(root, file))\n",
        "\n",
        "    print(f\"JSON 파일 발견: {len(all_json_files)}개\")\n",
        "\n",
        "    # JSON 파일명 -> 경로 딕셔너리 생성\n",
        "    label_dict = {}\n",
        "    for json_path in all_json_files:\n",
        "        base_name = os.path.splitext(os.path.basename(json_path))[0]\n",
        "        label_dict[base_name] = json_path\n",
        "\n",
        "    print(f\"JSON 딕셔너리 생성 완료: {len(label_dict)}개\")\n",
        "\n",
        "    # 각 split별로 매핑 생성\n",
        "    splits = ['train', 'valid', 'test']\n",
        "\n",
        "    for split in splits:\n",
        "        print(f\"\\n=== {split} 매핑 생성 중 ===\")\n",
        "\n",
        "        split_dir = os.path.join(dataset_dir, split)\n",
        "        if not os.path.exists(split_dir):\n",
        "            print(f\"{split} 폴더가 없습니다.\")\n",
        "            continue\n",
        "\n",
        "        # 해당 split의 파일들 가져오기\n",
        "        image_files = [f for f in os.listdir(split_dir)\n",
        "                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
        "\n",
        "        print(f\"{split} 파일 수: {len(image_files)}\")\n",
        "\n",
        "        # 매핑 딕셔너리 생성\n",
        "        split_mapping = {}\n",
        "        matched_count = 0\n",
        "\n",
        "        # 배치로 처리 (1000개씩)\n",
        "        batch_size = 1000\n",
        "        for i in range(0, len(image_files), batch_size):\n",
        "            batch_files = image_files[i:i+batch_size]\n",
        "\n",
        "            print(f\"  배치 {i//batch_size + 1}: {len(batch_files)}개 처리 중...\")\n",
        "\n",
        "            for image_file in batch_files:\n",
        "                # 파일명에서 JSON 파일명 추출\n",
        "                base_name = os.path.splitext(image_file)[0]\n",
        "\n",
        "                # 해당 JSON 파일이 있는지 확인\n",
        "                if base_name in label_dict:\n",
        "                    try:\n",
        "                        # JSON 파일 읽기\n",
        "                        with open(label_dict[base_name], 'r', encoding='utf-8-sig') as f:\n",
        "                            label_data = json.load(f)\n",
        "\n",
        "                        # 라벨 추출\n",
        "                        category_name = None\n",
        "                        quality = None\n",
        "\n",
        "                        # categories 딕셔너리 생성\n",
        "                        categories_dict = {}\n",
        "                        if 'categories' in label_data:\n",
        "                            for cat in label_data['categories']:\n",
        "                                categories_dict[cat['id']] = cat['name']\n",
        "\n",
        "                        # annotations에서 정보 추출\n",
        "                        if 'annotations' in label_data and len(label_data['annotations']) > 0:\n",
        "                            annotation = label_data['annotations'][0]\n",
        "\n",
        "                            if 'category_id' in annotation and annotation['category_id'] in categories_dict:\n",
        "                                category_name = categories_dict[annotation['category_id']]\n",
        "\n",
        "                            if 'attributes' in annotation and 'quality' in annotation['attributes']:\n",
        "                                quality = annotation['attributes']['quality']\n",
        "\n",
        "                        # 라벨 생성\n",
        "                        if category_name and quality:\n",
        "                            label = f\"{category_name}_{quality}\"\n",
        "                            split_mapping[image_file] = label\n",
        "                            matched_count += 1\n",
        "\n",
        "                    except Exception as e:\n",
        "                        # JSON 읽기 실패시 무시\n",
        "                        pass\n",
        "\n",
        "        print(f\"  처리 완료: {len(image_files)}개 중 {matched_count}개 매칭\")\n",
        "        print(f\"  매칭률: {matched_count/len(image_files)*100:.1f}%\")\n",
        "\n",
        "        # 매핑 파일을 로컬에 저장 (Drive 용량 절약)\n",
        "        mapping_file_path = f'/content/{split}_mapping.json'\n",
        "        with open(mapping_file_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(split_mapping, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"  매핑 파일 저장: {mapping_file_path}\")\n",
        "\n",
        "        # 매칭된 클래스들 확인\n",
        "        matched_classes = set(split_mapping.values())\n",
        "        print(f\"  발견된 클래스 수: {len(matched_classes)}\")\n",
        "        if matched_classes:\n",
        "            print(f\"  클래스 예시: {list(matched_classes)[:3]}\")\n",
        "\n",
        "    print(\"\\n매핑 파일 생성 완료!\")\n",
        "\n",
        "# 실행 함수\n",
        "def run_preprocessing(paired_data, class_names):\n",
        "    \"\"\"전체 전처리 실행\"\"\"\n",
        "    print(\"=== 데이터 전처리 시작 ===\")\n",
        "\n",
        "    # 1. 디렉토리 생성\n",
        "    create_directories()\n",
        "\n",
        "    # 2. 데이터셋 분할 및 복사\n",
        "    train_count, valid_count, test_count = split_and_copy_dataset(paired_data, class_names)\n",
        "\n",
        "    # 3. 매핑 파일 생성\n",
        "    create_mapping_files_v3()\n",
        "\n",
        "    print(f\"\\n=== 전처리 완료 ===\")\n",
        "    print(f\"Train: {train_count}, Valid: {valid_count}, Test: {test_count}\")\n",
        "\n",
        "    return train_count, valid_count, test_count\n",
        "\n",
        "# 실행 (이전 EDA에서 나온 paired_data, class_names 필요)\n",
        "if __name__ == \"__main__\":\n",
        "    # paired_data, class_names는 EDA에서 얻어야 함\n",
        "    train_count, valid_count, test_count = run_preprocessing(paired_data, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wd7fCewObk8"
      },
      "source": [
        "### 훈련 데이터가 없다고 판단한 원인 분석\n",
        ": 클래스 명과 실제 파일명이 매칭이 안됨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIKTdk5LfEsp",
        "outputId": "373674e9-a627-4335-e59d-3c9a2a30f16e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 실제 클래스명 vs 파일명 분석 ===\n",
            "실제 클래스명들:\n",
            "  고정 불량_불량품\n",
            "  고정 불량_양품\n",
            "  고정핀 불량_불량품\n",
            "  고정핀 불량_양품\n",
            "  단차_불량품\n",
            "\n",
            "파일명 예시:\n",
            "  202_201_20_b898f3ba-698a-465d-acc7-6552d243d4b4.jpg\n",
            "  202_201_20_35900030-01eb-499e-9de7-9cb82a324e6a.jpg\n",
            "  202_201_20_58ac6a9a-41d8-42c7-8f63-ac729e0a8076.jpg\n",
            "\n",
            "클래스명 매칭 테스트:\n",
            "  202_201_20_b898f3ba-698a-465d-acc7-6552d243d4b4.jpg → 매칭 안됨 ❌\n",
            "  202_201_20_35900030-01eb-499e-9de7-9cb82a324e6a.jpg → 매칭 안됨 ❌\n",
            "  202_201_20_58ac6a9a-41d8-42c7-8f63-ac729e0a8076.jpg → 매칭 안됨 ❌\n"
          ]
        }
      ],
      "source": [
        "# 실제 클래스명들 확인\n",
        "train_dir = '/content/drive/MyDrive/의장공정/데이터/image_classification/dataset/train'\n",
        "files = os.listdir(train_dir)\n",
        "\n",
        "print(\"=== 실제 클래스명 vs 파일명 분석 ===\")\n",
        "\n",
        "# 실제 생성된 클래스명들 (로그에서 확인됨)\n",
        "actual_class_names = [\n",
        "    '고정 불량_불량품', '고정 불량_양품', '고정핀 불량_불량품', '고정핀 불량_양품',\n",
        "    '단차_불량품', '단차_양품', '스크래치_불량품', '스크래치_양품',\n",
        "    '실링 불량_불량품', '실링 불량_양품', '연계 불량_불량품', '연계 불량_양품',\n",
        "    '외관 손상_불량품', '외관 손상_양품', '유격 불량_불량품', '유격 불량_양품'\n",
        "]\n",
        "\n",
        "print(\"실제 클래스명들:\")\n",
        "for name in actual_class_names[:5]:\n",
        "    print(f\"  {name}\")\n",
        "\n",
        "print(f\"\\n파일명 예시:\")\n",
        "for file in files[:3]:\n",
        "    print(f\"  {file}\")\n",
        "\n",
        "print(f\"\\n클래스명 매칭 테스트:\")\n",
        "for file in files[:3]:\n",
        "    base_name = os.path.splitext(file)[0]\n",
        "    matched = False\n",
        "\n",
        "    for class_name in actual_class_names:\n",
        "        if class_name in base_name:\n",
        "            print(f\"  {file} → {class_name} 매칭됨!\")\n",
        "            matched = True\n",
        "            break\n",
        "\n",
        "    if not matched:\n",
        "        print(f\"  {file} → 매칭 안됨 ❌\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmBW6VxbOfPL"
      },
      "source": [
        "### 해결: 매핑 파일 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXDHN3esKPLl",
        "outputId": "65fb9a72-b31c-431f-8a13-c1248e1e292d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "올바른 JSON 경로로 매핑 파일 재생성 시작...\n",
            "JSON 파일 탐색 중...\n",
            "JSON 파일 발견: 21553개\n",
            "JSON 딕셔너리 생성 완료: 21553개\n",
            "\n",
            "=== train 매핑 생성 중 ===\n",
            "train 파일 수: 15076\n",
            "  배치 1: 1000개 처리 중...\n",
            "  배치 2: 1000개 처리 중...\n",
            "  배치 3: 1000개 처리 중...\n",
            "  배치 4: 1000개 처리 중...\n",
            "  배치 5: 1000개 처리 중...\n",
            "  배치 6: 1000개 처리 중...\n",
            "  배치 7: 1000개 처리 중...\n",
            "  배치 8: 1000개 처리 중...\n",
            "  배치 9: 1000개 처리 중...\n",
            "  배치 10: 1000개 처리 중...\n",
            "  배치 11: 1000개 처리 중...\n",
            "  배치 12: 1000개 처리 중...\n",
            "  배치 13: 1000개 처리 중...\n",
            "  배치 14: 1000개 처리 중...\n",
            "  배치 15: 1000개 처리 중...\n",
            "  배치 16: 76개 처리 중...\n",
            "  처리 완료: 15076개 중 15076개 매칭\n",
            "  매칭률: 100.0%\n",
            "  매핑 파일 저장: /content/train_mapping.json\n",
            "  발견된 클래스 수: 24\n",
            "  클래스 예시: ['실링 불량_불량품', '헤밍 불량_양품', '고정 불량_불량품']\n",
            "\n",
            "=== valid 매핑 생성 중 ===\n",
            "valid 파일 수: 3232\n",
            "  배치 1: 1000개 처리 중...\n",
            "  배치 2: 1000개 처리 중...\n",
            "  배치 3: 1000개 처리 중...\n",
            "  배치 4: 232개 처리 중...\n",
            "  처리 완료: 3232개 중 3232개 매칭\n",
            "  매칭률: 100.0%\n",
            "  매핑 파일 저장: /content/valid_mapping.json\n",
            "  발견된 클래스 수: 24\n",
            "  클래스 예시: ['실링 불량_불량품', '헤밍 불량_양품', '고정 불량_불량품']\n",
            "\n",
            "=== test 매핑 생성 중 ===\n",
            "test 파일 수: 3245\n",
            "  배치 1: 1000개 처리 중...\n",
            "  배치 2: 1000개 처리 중...\n",
            "  배치 3: 1000개 처리 중...\n",
            "  배치 4: 245개 처리 중...\n",
            "  처리 완료: 3245개 중 3245개 매칭\n",
            "  매칭률: 100.0%\n",
            "  매핑 파일 저장: /content/test_mapping.json\n",
            "  발견된 클래스 수: 24\n",
            "  클래스 예시: ['실링 불량_불량품', '헤밍 불량_양품', '고정 불량_불량품']\n",
            "\n",
            "매핑 파일 생성 완료!\n"
          ]
        }
      ],
      "source": [
        "# 올바른 JSON 경로로 매핑 파일 다시 생성\n",
        "import os\n",
        "import json\n",
        "\n",
        "def create_mapping_files_v3():\n",
        "    \"\"\"전체 폴더를 탐색해서 JSON 파일 찾고 매핑 생성\"\"\"\n",
        "\n",
        "    # 경로 설정\n",
        "    dataset_dir = '/content/drive/MyDrive/의장공정/데이터/image_classification/dataset'\n",
        "    validation_dir = '/content/drive/MyDrive/의장공정/데이터/Validation'\n",
        "\n",
        "    print(\"올바른 JSON 경로로 매핑 파일 재생성 시작...\")\n",
        "\n",
        "    # 모든 JSON 파일 찾기\n",
        "    print(\"JSON 파일 탐색 중...\")\n",
        "    all_json_files = []\n",
        "    for root, dirs, files in os.walk(validation_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.json'):\n",
        "                all_json_files.append(os.path.join(root, file))\n",
        "\n",
        "    print(f\"JSON 파일 발견: {len(all_json_files)}개\")\n",
        "\n",
        "    # JSON 파일명 -> 경로 딕셔너리 생성\n",
        "    label_dict = {}\n",
        "    for json_path in all_json_files:\n",
        "        base_name = os.path.splitext(os.path.basename(json_path))[0]\n",
        "        label_dict[base_name] = json_path\n",
        "\n",
        "    print(f\"JSON 딕셔너리 생성 완료: {len(label_dict)}개\")\n",
        "\n",
        "    # 각 split별로 매핑 생성\n",
        "    splits = ['train', 'valid', 'test']\n",
        "\n",
        "    for split in splits:\n",
        "        print(f\"\\n=== {split} 매핑 생성 중 ===\")\n",
        "\n",
        "        split_dir = os.path.join(dataset_dir, split)\n",
        "        if not os.path.exists(split_dir):\n",
        "            print(f\"{split} 폴더가 없습니다.\")\n",
        "            continue\n",
        "\n",
        "        # 해당 split의 파일들 가져오기\n",
        "        image_files = [f for f in os.listdir(split_dir)\n",
        "                      if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
        "\n",
        "        print(f\"{split} 파일 수: {len(image_files)}\")\n",
        "\n",
        "        # 매핑 딕셔너리 생성\n",
        "        split_mapping = {}\n",
        "        matched_count = 0\n",
        "\n",
        "        # 배치로 처리 (1000개씩)\n",
        "        batch_size = 1000\n",
        "        for i in range(0, len(image_files), batch_size):\n",
        "            batch_files = image_files[i:i+batch_size]\n",
        "\n",
        "            print(f\"  배치 {i//batch_size + 1}: {len(batch_files)}개 처리 중...\")\n",
        "\n",
        "            for image_file in batch_files:\n",
        "                # 파일명에서 JSON 파일명 추출\n",
        "                base_name = os.path.splitext(image_file)[0]\n",
        "\n",
        "                # 해당 JSON 파일이 있는지 확인\n",
        "                if base_name in label_dict:\n",
        "                    try:\n",
        "                        # JSON 파일 읽기\n",
        "                        with open(label_dict[base_name], 'r', encoding='utf-8-sig') as f:\n",
        "                            label_data = json.load(f)\n",
        "\n",
        "                        # 라벨 추출\n",
        "                        category_name = None\n",
        "                        quality = None\n",
        "\n",
        "                        # categories 딕셔너리 생성\n",
        "                        categories_dict = {}\n",
        "                        if 'categories' in label_data:\n",
        "                            for cat in label_data['categories']:\n",
        "                                categories_dict[cat['id']] = cat['name']\n",
        "\n",
        "                        # annotations에서 정보 추출\n",
        "                        if 'annotations' in label_data and len(label_data['annotations']) > 0:\n",
        "                            annotation = label_data['annotations'][0]\n",
        "\n",
        "                            if 'category_id' in annotation and annotation['category_id'] in categories_dict:\n",
        "                                category_name = categories_dict[annotation['category_id']]\n",
        "\n",
        "                            if 'attributes' in annotation and 'quality' in annotation['attributes']:\n",
        "                                quality = annotation['attributes']['quality']\n",
        "\n",
        "                        # 라벨 생성\n",
        "                        if category_name and quality:\n",
        "                            label = f\"{category_name}_{quality}\"\n",
        "                            split_mapping[image_file] = label\n",
        "                            matched_count += 1\n",
        "\n",
        "                    except Exception as e:\n",
        "                        # JSON 읽기 실패시 무시\n",
        "                        pass\n",
        "\n",
        "        print(f\"  처리 완료: {len(image_files)}개 중 {matched_count}개 매칭\")\n",
        "        print(f\"  매칭률: {matched_count/len(image_files)*100:.1f}%\")\n",
        "\n",
        "        # 매핑 파일을 로컬에 저장 (Drive 용량 절약)\n",
        "        mapping_file_path = f'/content/{split}_mapping.json'\n",
        "        with open(mapping_file_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(split_mapping, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"  매핑 파일 저장: {mapping_file_path}\")\n",
        "\n",
        "        # 매칭된 클래스들 확인\n",
        "        matched_classes = set(split_mapping.values())\n",
        "        print(f\"  발견된 클래스 수: {len(matched_classes)}\")\n",
        "        if matched_classes:\n",
        "            print(f\"  클래스 예시: {list(matched_classes)[:3]}\")\n",
        "\n",
        "    print(\"\\n매핑 파일 생성 완료!\")\n",
        "\n",
        "# 실행\n",
        "create_mapping_files_v3()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6UofkaWWxCv",
        "outputId": "ed9a51d2-648d-483a-e39b-d4f744b8d7ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 현재 복사된 파일 개수 확인 ===\n",
            "train: 15,076개\n",
            "  예시 파일명:\n",
            "    202_201_20_b898f3ba-698a-465d-acc7-6552d243d4b4.jpg\n",
            "    202_201_20_35900030-01eb-499e-9de7-9cb82a324e6a.jpg\n",
            "    202_201_20_58ac6a9a-41d8-42c7-8f63-ac729e0a8076.jpg\n",
            "\n",
            "valid: 3,232개\n",
            "  예시 파일명:\n",
            "    202_201_20_91b4fad8-7837-460a-a5ef-ff7fedeae580.jpg\n",
            "    202_201_20_080f058e-19df-48c5-97c6-ed3aa19f430b.jpg\n",
            "    202_201_20_68be6931-2aab-458d-8668-ec22c1c4dc96.jpg\n",
            "\n",
            "test: 3,245개\n",
            "  예시 파일명:\n",
            "    202_201_20_394cc5ab-04a1-45c5-8246-14820e52399c.jpg\n",
            "    202_201_20_af702c24-c9f0-419c-9f3f-9626be95f50e.jpg\n",
            "    202_201_20_2249e3d8-2f9e-45ea-9f21-e59cebc8c4c9.jpg\n",
            "\n",
            "전체 복사된 파일: 21,553개\n",
            "예상 개수: 21,553개\n",
            "차이: 0개\n"
          ]
        }
      ],
      "source": [
        "# 현재 복사된 파일 개수 확인\n",
        "import os\n",
        "\n",
        "dataset_dir = '/content/drive/MyDrive/의장공정/데이터/image_classification/dataset'\n",
        "\n",
        "splits = ['train', 'valid', 'test']\n",
        "\n",
        "print(\"=== 현재 복사된 파일 개수 확인 ===\")\n",
        "total_files = 0\n",
        "\n",
        "for split in splits:\n",
        "    split_dir = os.path.join(dataset_dir, split)\n",
        "\n",
        "    if os.path.exists(split_dir):\n",
        "        files = [f for f in os.listdir(split_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
        "        file_count = len(files)\n",
        "        total_files += file_count\n",
        "\n",
        "        print(f\"{split}: {file_count:,}개\")\n",
        "\n",
        "        # 첫 3개 파일명 확인\n",
        "        if files:\n",
        "            print(f\"  예시 파일명:\")\n",
        "            for i, file in enumerate(files[:3]):\n",
        "                print(f\"    {file}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"{split}: 폴더 없음\")\n",
        "\n",
        "print(f\"전체 복사된 파일: {total_files:,}개\")\n",
        "\n",
        "# 예상된 개수와 비교\n",
        "expected_total = 15076 + 3232 + 3245  # 21,553개\n",
        "print(f\"예상 개수: {expected_total:,}개\")\n",
        "print(f\"차이: {total_files - expected_total:,}개\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBM8idN1bHGm"
      },
      "source": [
        "매핑 파일 생성 완료 ✔️\n",
        "ImageDataSet 클래스 수정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqpy7qMOaXuK",
        "outputId": "3ded9130-bfbd-44b6-cc1f-23f482ea8cc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset: 15076 samples\n",
            "Valid dataset: 3232 samples\n",
            "Test dataset: 3245 samples\n"
          ]
        }
      ],
      "source": [
        "# 매핑 파일을 사용하는 수정된 ImageDataset 클래스\n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class ImageDatasetWithMapping(Dataset):\n",
        "    def __init__(self, data_dir, mapping_file_path, class_names, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.class_names = class_names\n",
        "        self.transform = transform\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_names)}\n",
        "\n",
        "        # 매핑 파일 로드\n",
        "        with open(mapping_file_path, 'r', encoding='utf-8') as f:\n",
        "            self.filename_to_label = json.load(f)\n",
        "\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        self._load_from_mapping()\n",
        "\n",
        "    def _load_from_mapping(self):\n",
        "        \"\"\"매핑 파일을 사용해서 이미지와 라벨 로드\"\"\"\n",
        "        valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
        "\n",
        "        for file_name in os.listdir(self.data_dir):\n",
        "            if file_name.lower().endswith(valid_extensions):\n",
        "                # 매핑 파일에서 라벨 찾기\n",
        "                if file_name in self.filename_to_label:\n",
        "                    label = self.filename_to_label[file_name]\n",
        "\n",
        "                    # 클래스 인덱스 확인\n",
        "                    if label in self.class_to_idx:\n",
        "                        image_path = os.path.join(self.data_dir, file_name)\n",
        "                        self.image_paths.append(image_path)\n",
        "                        self.labels.append(self.class_to_idx[label])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "            # 에러 발생시 기본 이미지 생성\n",
        "            image = Image.new('RGB', (224, 224), color='white')\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# 수정된 create_data_loaders 함수\n",
        "def create_data_loaders_with_mapping(class_names):\n",
        "    \"\"\"매핑 파일을 사용하는 데이터 로더 생성 함수\"\"\"\n",
        "    from torchvision import transforms\n",
        "    from torch.utils.data import DataLoader\n",
        "    import torch\n",
        "\n",
        "    # Config 클래스 (기존과 동일)\n",
        "    dataset_dir = '/content/drive/MyDrive/의장공정/데이터/image_classification/dataset'\n",
        "\n",
        "    # 데이터 변환 정의\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomCrop((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # 데이터셋 생성\n",
        "    loaders = {}\n",
        "\n",
        "    for split in ['train', 'valid', 'test']:\n",
        "        split_dir = os.path.join(dataset_dir, split)\n",
        "        mapping_file_path = f'/content/{split}_mapping.json'\n",
        "\n",
        "        if os.path.exists(split_dir) and os.path.exists(mapping_file_path):\n",
        "            dataset = ImageDatasetWithMapping(\n",
        "                data_dir=split_dir,\n",
        "                mapping_file_path=mapping_file_path,\n",
        "                class_names=class_names,\n",
        "                transform=train_transform if split == 'train' else test_transform\n",
        "            )\n",
        "\n",
        "            if len(dataset) > 0:\n",
        "                loaders[split] = DataLoader(\n",
        "                    dataset,\n",
        "                    batch_size=32,\n",
        "                    shuffle=(split == 'train'),\n",
        "                    num_workers=2,\n",
        "                    pin_memory=True if torch.cuda.is_available() else False\n",
        "                )\n",
        "\n",
        "                print(f\"{split.capitalize()} dataset: {len(dataset)} samples\")\n",
        "\n",
        "    return loaders.get('train'), loaders.get('valid'), loaders.get('test')\n",
        "\n",
        "train_loader, valid_loader, test_loader = create_data_loaders_with_mapping(class_names)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
